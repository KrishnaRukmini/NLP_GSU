{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuEmCpyD02I2",
        "outputId": "0c1320ef-131a-4527-cc0f-a6dd5bef11be"
      },
      "source": [
        "!unzip \"/content/trainingandtestdata.zip\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/trainingandtestdata.zip\n",
            "  inflating: testdata.manual.2009.06.14.csv  \n",
            "  inflating: training.1600000.processed.noemoticon.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93rihvHL5ZXb"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "m_cols = ['polarity','id','date','query','user','text']\n",
        "\n",
        "train_data = pd.read_csv(\"/content/training.1600000.processed.noemoticon.csv\", names = m_cols , encoding='latin-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "-tAdpTHo58ny",
        "outputId": "23ea65ad-0306-4a2b-ccd1-7469410521ec"
      },
      "source": [
        "train_data.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   polarity  ...                                               text\n",
              "0         0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1         0  ...  is upset that he can't update his Facebook by ...\n",
              "2         0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3         0  ...    my whole body feels itchy and like its on fire \n",
              "4         0  ...  @nationwideclass no, it's not behaving at all....\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMS-JGxJ8xez"
      },
      "source": [
        "1. Take the positive and the negative tweets only. Use Sklearn to split the dataset in 80%\n",
        "training, 20% testing splits. Provide a nicely formatted summary of these splits,\n",
        "containing their size) (15 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWCJoQQt6N1M"
      },
      "source": [
        "negative_df = train_data[train_data['polarity'] == 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSYLmwxL7Zmv"
      },
      "source": [
        "positive_df = train_data[train_data['polarity'] == 4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "367Gug9b9OlH"
      },
      "source": [
        "tweets_neg_pos_df = pd.concat([negative_df,positive_df], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "rpNR3L2a9bxW",
        "outputId": "d5f3aebb-0f28-44a9-82bf-48af31bc6cc8"
      },
      "source": [
        "tweets_neg_pos_df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   polarity  ...                                               text\n",
              "0         0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1         0  ...  is upset that he can't update his Facebook by ...\n",
              "2         0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3         0  ...    my whole body feels itchy and like its on fire \n",
              "4         0  ...  @nationwideclass no, it's not behaving at all....\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJn5UgmW-CI6"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test ,y_train, y_test= train_test_split(tweets_neg_pos_df.text,tweets_neg_pos_df.polarity, test_size=0.2, random_state=2361)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8_zZzy5DBnJ",
        "outputId": "b4d18001-ac6d-4d1f-de2d-b7ec3ffa6e2f"
      },
      "source": [
        "print(\"Shape Training tweet Data: \" + str(x_train.shape))\n",
        "print(\"Shape Training polarity Data: \" + str(y_train.shape))\n",
        "print(\"Shape Test tweet Data: \" + str(x_test.shape))\n",
        "print(\"Shape Test polarity Data: \" + str(y_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape Training tweet Data: (1280000,)\n",
            "Shape Training polarity Data: (1280000,)\n",
            "Shape Test tweet Data: (320000,)\n",
            "Shape Test polarity Data: (320000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIv301cpBO2s"
      },
      "source": [
        "2) Use the code from the previous classes to build the following models (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC1otEiEBcPd"
      },
      "source": [
        "a) SVM using TF-IDF\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq7-vZ5TAWu6"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "def create_model(algorithm,vectorization,x_train_data,y_train_data):\n",
        "  model = make_pipeline(vectorization,algorithm)\n",
        "  model = model.fit(x_train_data,y_train_data)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpMjMcfhB3qd"
      },
      "source": [
        "model_SVC_TF = create_model(LinearSVC(),TfidfVectorizer(),x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yGg8rMhBfpU"
      },
      "source": [
        "b) Naive Bayes using TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-rofgSGBjHa"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "model_Naive_TF = create_model(MultinomialNB(),TfidfVectorizer(),x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdYt0N-wBj7a"
      },
      "source": [
        "c) Random Forest using TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_mEw6DsBmOf"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier \n",
        "\n",
        "model_RFC_TF = create_model(RandomForestClassifier(max_depth = 30),TfidfVectorizer(),x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deEsOTyAMf8B"
      },
      "source": [
        "3) Use the code from the LSTM class to build a classifier for negative and positive\n",
        "sentiment tweets. Train the model with the training data split. Once the model is built,\n",
        "test it with the testing data split. Display the classifier report for this evaluation. Answer\n",
        "the following question: What can you say about the performance of this model? (40\n",
        "points)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN3atyiTMgko"
      },
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from numpy import array\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY4mn6so3aqw"
      },
      "source": [
        "lab_to_sentiment = {0:0, 4:1}\n",
        "def decode_label(label):\n",
        " return lab_to_sentiment[label]\n",
        "\n",
        "y_train = y_train.apply(lambda x: decode_label(x))\n",
        "y_test = y_test.apply(lambda x: decode_label(x))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gwmbYvQ73Lm",
        "outputId": "43171a4b-8a48-4dae-8982-b548d2462a22"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tweets_neg_pos_df.text)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary Size :\", vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size : 690961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUKvBRtQ76bn",
        "outputId": "c2fa0e7c-5dad-4f6a-90fa-64b6d1df9f54"
      },
      "source": [
        "\n",
        "print(\"Maximum tweet length: {}\".format(len(max((tweets_neg_pos_df.text), key=len))))\n",
        "print(\"Minimum tweet length: {}\".format(len(min((tweets_neg_pos_df.text), key=len))))\n",
        "result = [len(x) for x in tweets_neg_pos_df.text]\n",
        "print(\"Mean tweet length: {}\".format(np.mean(result)))\n",
        "\n",
        "\n",
        "print(\"\")\n",
        "print(\"Machine readable tweets\")\n",
        "print(\"  tweet Text: \" + str(x_train[60]))\n",
        "print(\"  tweet Sentiment: \" + str(y_train[60]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum tweet length: 374\n",
            "Minimum tweet length: 6\n",
            "Mean tweet length: 74.09011125\n",
            "\n",
            "Machine readable tweets\n",
            "  tweet Text: @BatManYNG I miss my ps3, it's out of commission  Wutcha playing? Have you copped 'Blood On The Sand'?\n",
            "  tweet Sentiment: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvziaApP8F3Q",
        "outputId": "c7624803-b587-401e-e7a8-983648eacb56"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "MAX_SEQUENCE_LENGTH = 374\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(x_train), maxlen = MAX_SEQUENCE_LENGTH)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(x_test), maxlen = MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print(\"Training X Shape:\",x_train.shape)\n",
        "print(\"Testing X Shape:\",x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training X Shape: (1280000, 374)\n",
            "Testing X Shape: (320000, 374)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsYm4ZZy8JSB",
        "outputId": "b7929080-b1d7-4f10-a383-b4361ec5d60e"
      },
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from numpy import array\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim = vocab_size, # The size of our vocabulary \n",
        "        output_dim = 32, # Dimensions to which each words shall be mapped\n",
        "        input_length = MAX_SEQUENCE_LENGTH # Length of input sequences\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.Dropout(\n",
        "        rate=0.25 # Randomly disable 25% of neurons\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.LSTM(\n",
        "        units=32 # 32 LSTM units in this layer\n",
        "    )\n",
        ")\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.Dropout(\n",
        "        rate=0.25 # Randomly disable 25% of neurons\n",
        "    )\n",
        ")\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.Dense(\n",
        "        units=1, # Single unit\n",
        "        activation='sigmoid' # Sigmoid activation function (output from 0 to 1)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.binary_crossentropy, # loss function\n",
        "    optimizer=tf.keras.optimizers.Adam(), # optimiser function\n",
        "    metrics=['accuracy']) # reporting metric\n",
        "\n",
        "# Display a summary of the models structure\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 374, 32)           22110752  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 374, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 22,119,105\n",
            "Trainable params: 22,119,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwtbRpxK8O1j",
        "outputId": "e46747c6-0f91-4e69-bd60-c5561dbba5e5"
      },
      "source": [
        "history = model.fit(x_train, y_train , batch_size=256, epochs=3,validation_split=0.2, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "4000/4000 [==============================] - 2287s 571ms/step - loss: 0.4721 - accuracy: 0.7736 - val_loss: 0.4010 - val_accuracy: 0.8168\n",
            "Epoch 2/3\n",
            "4000/4000 [==============================] - 2293s 573ms/step - loss: 0.3488 - accuracy: 0.8470 - val_loss: 0.4086 - val_accuracy: 0.8149\n",
            "Epoch 3/3\n",
            "4000/4000 [==============================] - 2214s 554ms/step - loss: 0.2737 - accuracy: 0.8858 - val_loss: 0.4322 - val_accuracy: 0.8081\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_zfBm0SnEJu"
      },
      "source": [
        "LSTM is way efficient which is evident through it's accuracy values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gdvMdya8VL5"
      },
      "source": [
        "import numpy\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "predicted_classes = model.predict_classes(x_test)\n",
        "print(classification_report(y_test, predicted_classes, target_names=[0,1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmGZ24C9_AHG"
      },
      "source": [
        "4. Compare all models together in terms of Precision, Recall and F1 score. Put all of\n",
        "these numbers in a nicely formatted dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "568E5TuthTxD"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test ,y_train, y_test= train_test_split(tweets_neg_pos_df.text,tweets_neg_pos_df.polarity, test_size=0.2, random_state=2361)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C94nSlzMy1X"
      },
      "source": [
        "label_SVC = model_SVC_TF.predict(x_test)\n",
        "label_Naive_TF = model_Naive_TF.predict(x_test)\n",
        "label_RFC_TF = model_RFC_TF.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKGpSFx78a3r"
      },
      "source": [
        "import sklearn\n",
        "F1_Scores = [sklearn.metrics.f1_score(label_SVC,y_test , average='macro'),\n",
        "             sklearn.metrics.f1_score(label_Naive_TF,y_test , average='macro'),\n",
        "             sklearn.metrics.f1_score(label_RFC_TF,y_test, average='macro')]\n",
        "\n",
        "Precision = [sklearn.metrics.precision_score(y_test,label_SVC, pos_label = 4),\n",
        "             sklearn.metrics.precision_score(y_test,label_Naive_TF, pos_label = 4),\n",
        "             sklearn.metrics.precision_score(y_test,label_RFC_TF, pos_label = 4)]\n",
        "\n",
        "Recall = [sklearn.metrics.recall_score(y_test,label_SVC,pos_label = 4),\n",
        "          sklearn.metrics.recall_score(y_test,label_Naive_TF,pos_label = 4),\n",
        "          sklearn.metrics.recall_score(y_test,label_RFC_TF,pos_label = 4)]\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBIeEwioDkAV"
      },
      "source": [
        "model_name = ['SVM','Naive','RFC']\n",
        "performance = pd.DataFrame()\n",
        "performance['Model_name'] = model_name\n",
        "performance['Precision'] = Precision\n",
        "performance['Recall'] = Recall\n",
        "performance['Macro_F1-Score'] = F1_Scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "sPayF7prhs1q",
        "outputId": "a2ac89d5-640d-406b-fd30-add1d9cc7aca"
      },
      "source": [
        "performance"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model_name</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Macro_F1-Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SVM</td>\n",
              "      <td>0.793902</td>\n",
              "      <td>0.800615</td>\n",
              "      <td>0.796381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Naive</td>\n",
              "      <td>0.800344</td>\n",
              "      <td>0.728078</td>\n",
              "      <td>0.772759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RFC</td>\n",
              "      <td>0.733351</td>\n",
              "      <td>0.769509</td>\n",
              "      <td>0.744698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Model_name  Precision    Recall  Macro_F1-Score\n",
              "0        SVM   0.793902  0.800615        0.796381\n",
              "1      Naive   0.800344  0.728078        0.772759\n",
              "2        RFC   0.733351  0.769509        0.744698"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZaDKkwwiCYM"
      },
      "source": [
        " Answer the following questions: Which model performs the best? Why do you think this is? What do you think you can do to\n",
        "improve performance?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "196C6Wd9hvNJ",
        "outputId": "65e51238-18fd-4282-f710-04a666c050cb"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "val_count = tweets_neg_pos_df.polarity.value_counts()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(val_count.index, val_count.values)\n",
        "plt.title(\"Sentiment Data Distribution\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Sentiment Data Distribution')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYLv0yDSk7s3"
      },
      "source": [
        "As we are comparing amongst the classifiers of a balanced dataset, Precision and recall are very much likely to be considered than F-score. Low precision and high recall rates are seen in SVM and RFC but based on values we can conclude that SVM is the better classifer amongst above three models.\n",
        "\n",
        "But when compared, LSTM is way efficient which is evident through it's accuracy values. \n",
        "\n",
        "yes, I do think we can improve performance by cleaning the tweets and avoiding noise data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTF8iS8Bnuxj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X1q7oPrFUM3"
      },
      "source": [
        "5. Add to the comparison of #4 a the manually calculated precision, recall and F1 score\n",
        "using VADER and their suggested defaults to categorize the test split tweets in positive\n",
        "or negative. Answer the following questions: Is this approach as good as the previous\n",
        "ones? Why do you think this is? (30 points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vgb_zfM5QGTg",
        "outputId": "2a656807-ccfa-4284-c740-66b6165e69a7"
      },
      "source": [
        "import nltk\n",
        "import numpy\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VIPjMe9VwB6"
      },
      "source": [
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "#Vader_df = tweets_neg_pos_df.copy()\n",
        "#Vader_df.drop(columns=['id','date','query','user'],axis = 0,inplace=True)\n",
        "\n",
        "Vader_df=pd.DataFrame(x_test,columns=['text'])\n",
        "\n",
        "compound = []\n",
        "pos=[]\n",
        "neu=[]\n",
        "neg = []\n",
        "\n",
        "for row in Vader_df['text']:\n",
        "  #print(row)\n",
        "  vs = sid.polarity_scores(row)\n",
        "\n",
        "  compound.append(vs['compound'])\n",
        "  pos.append(vs['pos'])\n",
        "  neu.append(vs['neu'])\n",
        "  neg.append(vs['neg'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VbJo-EUV85r"
      },
      "source": [
        "Vader_df['compound'] = compound\n",
        "Vader_df['pos'] = pos\n",
        "Vader_df['neu'] = neu\n",
        "Vader_df['neg'] = neg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "W-CtB5mQV82E",
        "outputId": "e93a7e20-61a0-4125-83aa-9bc985d05074"
      },
      "source": [
        "Vader_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>compound</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "      <th>neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>402563</th>\n",
              "      <td>is going to bed now after a day of hard work -...</td>\n",
              "      <td>0.2732</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.813</td>\n",
              "      <td>0.067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1558129</th>\n",
              "      <td>@lilxicanita aww mecheee you can be my friend....</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1415201</th>\n",
              "      <td>having my big brother open for me is a dream c...</td>\n",
              "      <td>0.5905</td>\n",
              "      <td>0.221</td>\n",
              "      <td>0.779</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1112565</th>\n",
              "      <td>acting going well. will reach goal by God's gr...</td>\n",
              "      <td>0.6124</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>706645</th>\n",
              "      <td>@morganmovement what?! Wtf happened?! I'm doin...</td>\n",
              "      <td>-0.6950</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.760</td>\n",
              "      <td>0.240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1087432</th>\n",
              "      <td>@mileycyrus http://twitpic.com/5ppwd - I have ...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1520516</th>\n",
              "      <td>@gin_lady thanks, lots of sleep and knitting s...</td>\n",
              "      <td>0.6808</td>\n",
              "      <td>0.444</td>\n",
              "      <td>0.556</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>728046</th>\n",
              "      <td>just watched like a three hour movie arizona i...</td>\n",
              "      <td>0.3612</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.839</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285604</th>\n",
              "      <td>I saw all the 'tussen de oren' episodes and f...</td>\n",
              "      <td>-0.7316</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.745</td>\n",
              "      <td>0.255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1311901</th>\n",
              "      <td>He say Im special.  *bllush</td>\n",
              "      <td>0.4019</td>\n",
              "      <td>0.403</td>\n",
              "      <td>0.597</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>320000 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      text  ...    neg\n",
              "402563   is going to bed now after a day of hard work -...  ...  0.067\n",
              "1558129  @lilxicanita aww mecheee you can be my friend....  ...  0.000\n",
              "1415201  having my big brother open for me is a dream c...  ...  0.000\n",
              "1112565  acting going well. will reach goal by God's gr...  ...  0.000\n",
              "706645   @morganmovement what?! Wtf happened?! I'm doin...  ...  0.240\n",
              "...                                                    ...  ...    ...\n",
              "1087432  @mileycyrus http://twitpic.com/5ppwd - I have ...  ...  0.000\n",
              "1520516  @gin_lady thanks, lots of sleep and knitting s...  ...  0.000\n",
              "728046   just watched like a three hour movie arizona i...  ...  0.000\n",
              "285604    I saw all the 'tussen de oren' episodes and f...  ...  0.255\n",
              "1311901                        He say Im special.  *bllush  ...  0.000\n",
              "\n",
              "[320000 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSMOfTpuV80G"
      },
      "source": [
        "default = 0.05\n",
        "sentiment = []\n",
        "for row in Vader_df['compound']:\n",
        "  if row >= default:\n",
        "    sentiment.append('Positive')\n",
        "  elif row < default:\n",
        "    sentiment.append('Negative')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYZJRLfaSTKi"
      },
      "source": [
        "Vader_df['sentiment'] = sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "93bTtL1TQzM1",
        "outputId": "b5b7d4ac-c8f4-43f3-b15d-c5d5415683fb"
      },
      "source": [
        "Vader_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>compound</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "      <th>neg</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>402563</th>\n",
              "      <td>is going to bed now after a day of hard work -...</td>\n",
              "      <td>0.2732</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.813</td>\n",
              "      <td>0.067</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1558129</th>\n",
              "      <td>@lilxicanita aww mecheee you can be my friend....</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1415201</th>\n",
              "      <td>having my big brother open for me is a dream c...</td>\n",
              "      <td>0.5905</td>\n",
              "      <td>0.221</td>\n",
              "      <td>0.779</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1112565</th>\n",
              "      <td>acting going well. will reach goal by God's gr...</td>\n",
              "      <td>0.6124</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>706645</th>\n",
              "      <td>@morganmovement what?! Wtf happened?! I'm doin...</td>\n",
              "      <td>-0.6950</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.760</td>\n",
              "      <td>0.240</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1087432</th>\n",
              "      <td>@mileycyrus http://twitpic.com/5ppwd - I have ...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1520516</th>\n",
              "      <td>@gin_lady thanks, lots of sleep and knitting s...</td>\n",
              "      <td>0.6808</td>\n",
              "      <td>0.444</td>\n",
              "      <td>0.556</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>728046</th>\n",
              "      <td>just watched like a three hour movie arizona i...</td>\n",
              "      <td>0.3612</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.839</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285604</th>\n",
              "      <td>I saw all the 'tussen de oren' episodes and f...</td>\n",
              "      <td>-0.7316</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.745</td>\n",
              "      <td>0.255</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1311901</th>\n",
              "      <td>He say Im special.  *bllush</td>\n",
              "      <td>0.4019</td>\n",
              "      <td>0.403</td>\n",
              "      <td>0.597</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>320000 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      text  ...  sentiment\n",
              "402563   is going to bed now after a day of hard work -...  ...   Positive\n",
              "1558129  @lilxicanita aww mecheee you can be my friend....  ...   Negative\n",
              "1415201  having my big brother open for me is a dream c...  ...   Positive\n",
              "1112565  acting going well. will reach goal by God's gr...  ...   Positive\n",
              "706645   @morganmovement what?! Wtf happened?! I'm doin...  ...   Negative\n",
              "...                                                    ...  ...        ...\n",
              "1087432  @mileycyrus http://twitpic.com/5ppwd - I have ...  ...   Negative\n",
              "1520516  @gin_lady thanks, lots of sleep and knitting s...  ...   Positive\n",
              "728046   just watched like a three hour movie arizona i...  ...   Positive\n",
              "285604    I saw all the 'tussen de oren' episodes and f...  ...   Negative\n",
              "1311901                        He say Im special.  *bllush  ...   Positive\n",
              "\n",
              "[320000 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w7SueJpn-3f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2UjwQzhle04",
        "outputId": "f8b4c79b-626e-46a3-be5b-da2c96e9b59c"
      },
      "source": [
        "y_pred = Vader_df.sentiment\n",
        "\n",
        "lab_to_sentiment = {0:\"Negative\", 4:\"Positive\"}\n",
        "def decode_label(label):\n",
        "  return lab_to_sentiment[label]\n",
        "\n",
        "y_test = y_test.apply(lambda x: decode_label(x))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, y_pred)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[109990,  50008],\n",
              "       [ 61642,  98360]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPmfcQIDoY2h"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "# Recall\n",
        "from sklearn.metrics import recall_score\n",
        "recall= recall_score(y_test, y_pred,pos_label='Positive')\n",
        "# Precision\n",
        "from sklearn.metrics import precision_score\n",
        "precision = precision_score(y_test, y_pred,pos_label='Positive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrbUOhINpUr1"
      },
      "source": [
        "F1 = 2 * (precision * recall) / (precision + recall)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9sgDHdytNdX",
        "outputId": "a0f6606a-6f73-487b-96e5-802c2f9b5a09"
      },
      "source": [
        "F1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6379349482764213"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZR0EmDHtvUd"
      },
      "source": [
        "Bonus (30 points): Try the following things to improve the LSTM model:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIq0ENs_tzUI"
      },
      "source": [
        "1) Use 90% training data, 10% testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGwqm4FEuLIL"
      },
      "source": [
        "x_trainn, x_testn ,y_trainn, y_testn= train_test_split(tweets_neg_pos_df.text,tweets_neg_pos_df.polarity, test_size=0.1, random_state=2361)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu9zD_80Fka5"
      },
      "source": [
        "lab_to_sentiment = {0:0, 4:1}\n",
        "def decode_label(label):\n",
        " return lab_to_sentiment[label]\n",
        "\n",
        "y_trainn = y_trainn.apply(lambda x: decode_label(x))\n",
        "y_testn = y_testn.apply(lambda x: decode_label(x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XX-blupFy1R",
        "outputId": "2e5d9faf-e3d1-46dd-dc55-1041274e9a50"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tweets_neg_pos_df.text)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary Size :\", vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size : 690961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IatTSzJJFypM",
        "outputId": "a7efeea8-72a2-4e8d-80d8-f5d486c1c9de"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "print(\"Maximum tweet length: {}\".format(len(max((tweets_neg_pos_df.text), key=len))))\n",
        "print(\"Minimum tweet length: {}\".format(len(min((tweets_neg_pos_df.text), key=len))))\n",
        "result = [len(x) for x in tweets_neg_pos_df.text]\n",
        "print(\"Mean tweet length: {}\".format(np.mean(result)))\n",
        "\n",
        "\n",
        "print(\"\")\n",
        "print(\"Machine readable tweets\")\n",
        "print(\"  tweet Text: \" + str(x_trainn[60]))\n",
        "print(\"  tweet Sentiment: \" + str(y_trainn[60]))\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "MAX_SEQUENCE_LENGTH = 374\n",
        "x_trainn = pad_sequences(tokenizer.texts_to_sequences(x_trainn), maxlen = MAX_SEQUENCE_LENGTH)\n",
        "x_testn = pad_sequences(tokenizer.texts_to_sequences(x_testn), maxlen = MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print(\"Training X Shape:\",x_trainn.shape)\n",
        "print(\"Testing X Shape:\",x_testn.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum tweet length: 374\n",
            "Minimum tweet length: 6\n",
            "Mean tweet length: 74.09011125\n",
            "\n",
            "Machine readable tweets\n",
            "  tweet Text: @BatManYNG I miss my ps3, it's out of commission  Wutcha playing? Have you copped 'Blood On The Sand'?\n",
            "  tweet Sentiment: 0\n",
            "Training X Shape: (1440000, 374)\n",
            "Testing X Shape: (160000, 374)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_d0f1alFydq",
        "outputId": "ef4c49af-2d93-423d-c313-2fefd2b20745"
      },
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from numpy import array\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim = vocab_size, # The size of our vocabulary \n",
        "        output_dim = 32, # Dimensions to which each words shall be mapped\n",
        "        input_length = MAX_SEQUENCE_LENGTH # Length of input sequences\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.Dropout(\n",
        "        rate=0.25 # Randomly disable 25% of neurons\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.LSTM(\n",
        "        units=32 # 32 LSTM units in this layer\n",
        "    )\n",
        ")\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.Dropout(\n",
        "        rate=0.25 # Randomly disable 25% of neurons\n",
        "    )\n",
        ")\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.Dense(\n",
        "        units=1, # Single unit\n",
        "        activation='sigmoid' # Sigmoid activation function (output from 0 to 1)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.binary_crossentropy, # loss function\n",
        "    optimizer=tf.keras.optimizers.Adam(), # optimiser function\n",
        "    metrics=['accuracy']) # reporting metric\n",
        "\n",
        "# Display a summary of the models structure\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 374, 32)           22110752  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 374, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 22,119,105\n",
            "Trainable params: 22,119,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPn84bw4FyOx",
        "outputId": "b6d96cad-3bfe-4260-cd80-0d055d550cc9"
      },
      "source": [
        "history = model.fit(x_trainn, y_trainn , batch_size=256, epochs=3,validation_split=0.2, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "4500/4500 [==============================] - 1815s 403ms/step - loss: 0.4686 - accuracy: 0.7752 - val_loss: 0.3954 - val_accuracy: 0.8200\n",
            "Epoch 2/3\n",
            "4500/4500 [==============================] - 1803s 401ms/step - loss: 0.3464 - accuracy: 0.8486 - val_loss: 0.4075 - val_accuracy: 0.8159\n",
            "Epoch 3/3\n",
            "4500/4500 [==============================] - 1821s 405ms/step - loss: 0.2763 - accuracy: 0.8849 - val_loss: 0.4315 - val_accuracy: 0.8097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piddEsvct5Ba"
      },
      "source": [
        "2) Remove stopwords from the tweets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pv7gM_-TvNcj",
        "outputId": "704e8eea-1cce-4cba-ac86-d2d5a572c2b6"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXAMyKoo1z3r"
      },
      "source": [
        "tweets_neg_pos_df['tweets_no_stopwords'] = tweets_neg_pos_df['text'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "momGmirfn_LC",
        "outputId": "9353463e-0bed-4ab0-b2e3-bb971b6d05ce"
      },
      "source": [
        "\n",
        "print(\"Maximum tweet length: {}\".format(len(max((tweets_neg_pos_df.text), key=len))))\n",
        "print(\"Minimum tweet length: {}\".format(len(min((tweets_neg_pos_df.text), key=len))))\n",
        "result = [len(x) for x in tweets_neg_pos_df.text]\n",
        "print(\"Mean tweet length: {}\".format(np.mean(result)))\n",
        "\n",
        "\n",
        "print(\"\")\n",
        "print(\"Machine readable tweets\")\n",
        "print(\"  tweet Text: \" + str(x_train[60]))\n",
        "print(\"  tweet Sentiment: \" + str(y_train[60]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum tweet length: 374\n",
            "Minimum tweet length: 6\n",
            "Mean tweet length: 74.09011125\n",
            "\n",
            "Machine readable tweets\n",
            "  tweet Text: @BatManYNG I miss my ps3, it's out of commission  Wutcha playing? Have you copped 'Blood On The Sand'?\n",
            "  tweet Sentiment: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPWKiQxqn_LC",
        "outputId": "0b5cba15-7933-45a3-bc70-0c06249b57b9"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "MAX_SEQUENCE_LENGTH = 374\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(x_train), maxlen = MAX_SEQUENCE_LENGTH)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(x_test), maxlen = MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print(\"Training X Shape:\",x_train.shape)\n",
        "print(\"Testing X Shape:\",x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training X Shape: (1280000, 374)\n",
            "Testing X Shape: (320000, 374)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHEfeXnun_LD",
        "outputId": "0e86ee6c-61a8-4810-95b8-e5e75d87b5df"
      },
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from numpy import array\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim = vocab_size, # The size of our vocabulary \n",
        "        output_dim = 32, # Dimensions to which each words shall be mapped\n",
        "        input_length = MAX_SEQUENCE_LENGTH # Length of input sequences\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.Dropout(\n",
        "        rate=0.25 # Randomly disable 25% of neurons\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.LSTM(\n",
        "        units=32 # 32 LSTM units in this layer\n",
        "    )\n",
        ")\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.Dropout(\n",
        "        rate=0.25 # Randomly disable 25% of neurons\n",
        "    )\n",
        ")\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.Dense(\n",
        "        units=1, # Single unit\n",
        "        activation='sigmoid' # Sigmoid activation function (output from 0 to 1)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.binary_crossentropy, # loss function\n",
        "    optimizer=tf.keras.optimizers.Adam(), # optimiser function\n",
        "    metrics=['accuracy']) # reporting metric\n",
        "\n",
        "# Display a summary of the models structure\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 374, 32)           22110752  \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 374, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 22,119,105\n",
            "Trainable params: 22,119,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7CcOFjnn_LE",
        "outputId": "73bc9059-8afd-41d7-de03-35d101ecb79f"
      },
      "source": [
        "history = model.fit(x_train, y_train , batch_size=256, epochs=3,validation_split=0.2, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "4000/4000 [==============================] - 1656s 414ms/step - loss: 0.4725 - accuracy: 0.7733 - val_loss: 0.4039 - val_accuracy: 0.8162\n",
            "Epoch 2/3\n",
            "1825/4000 [============>.................] - ETA: 14:00 - loss: 0.3512 - accuracy: 0.8452"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emFUpjaQuJyG"
      },
      "source": [
        "3) Remove all user mentions for the tweets (@something)\n",
        "Compare all three new models in terms of their precision, recall and F1 score. Answer the\n",
        "following questions: Did this change the results in any way? Why do you think so"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pvdEjD3oBMr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJGtDfLGUPvy"
      },
      "source": [
        "3) Remove all user mentions for the tweets (@something) Compare all three new models in terms of their precision, recall and F1 score. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWc98cicUEuF"
      },
      "source": [
        "tweets_no_user_mentions = []\n",
        "import re\n",
        "\n",
        "for unwanted in tweets_neg_pos_df.text :\n",
        "    #print(unwanted)\n",
        "    fixed_doc = re.sub(r\"@(\\w+)\", '', unwanted)\n",
        "    tweets_no_user_mentions.append(str(fixed_doc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U52yDnf7bh1a"
      },
      "source": [
        "tweets_neg_pos_df['tweets_no_user_mentions'] = tweets_no_user_mentions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrwjWHSLfGLi",
        "outputId": "544cb257-4e35-4da6-cf89-2647814962e2"
      },
      "source": [
        "tweets_neg_pos_df['tweets_no_user_mentions'].head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     http://twitpic.com/2y1zl - Awww, that's a bum...\n",
              "1    is upset that he can't update his Facebook by ...\n",
              "2     I dived many times for the ball. Managed to s...\n",
              "3      my whole body feels itchy and like its on fire \n",
              "4     no, it's not behaving at all. i'm mad. why am...\n",
              "Name: tweets_no_user_mentions, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnEzSwrxoJQZ"
      },
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from numpy import array\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyZjecjOffet"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test ,y_train, y_test= train_test_split(tweets_neg_pos_df.tweets_no_user_mentions,tweets_neg_pos_df.polarity, test_size=0.2, random_state=2361)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlsYjEBSoJQa"
      },
      "source": [
        "lab_to_sentiment = {0:0, 4:1}\n",
        "def decode_label(label):\n",
        " return lab_to_sentiment[label]\n",
        "\n",
        "y_train = y_train.apply(lambda x: decode_label(x))\n",
        "y_test = y_test.apply(lambda x: decode_label(x))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGBFqv0ooJQb",
        "outputId": "10ee8f8a-a28c-4c87-d760-9588e57ca5e4"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tweets_neg_pos_df.text)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary Size :\", vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size : 690961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO1w6EmMoJQc",
        "outputId": "44dd4271-0b94-4e8a-b77e-50949340dd94"
      },
      "source": [
        "\n",
        "print(\"Maximum tweet length: {}\".format(len(max((tweets_neg_pos_df.text), key=len))))\n",
        "print(\"Minimum tweet length: {}\".format(len(min((tweets_neg_pos_df.text), key=len))))\n",
        "result = [len(x) for x in tweets_neg_pos_df.text]\n",
        "print(\"Mean tweet length: {}\".format(np.mean(result)))\n",
        "\n",
        "\n",
        "print(\"\")\n",
        "print(\"Machine readable tweets\")\n",
        "print(\"  tweet Text: \" + str(x_train[60]))\n",
        "print(\"  tweet Sentiment: \" + str(y_train[60]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum tweet length: 374\n",
            "Minimum tweet length: 6\n",
            "Mean tweet length: 74.09011125\n",
            "\n",
            "Machine readable tweets\n",
            "  tweet Text:  I miss my ps3, it's out of commission  Wutcha playing? Have you copped 'Blood On The Sand'?\n",
            "  tweet Sentiment: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSRZTNFPoJQg",
        "outputId": "d5226dfc-183b-4895-8427-922051f32e61"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "MAX_SEQUENCE_LENGTH = 374\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(x_train), maxlen = MAX_SEQUENCE_LENGTH)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(x_test), maxlen = MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print(\"Training X Shape:\",x_train.shape)\n",
        "print(\"Testing X Shape:\",x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training X Shape: (1280000, 374)\n",
            "Testing X Shape: (320000, 374)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYNm-Q3joJQj",
        "outputId": "b075ec4e-abcb-4b93-d3e2-14e3fdbe1694"
      },
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from numpy import array\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim = vocab_size, # The size of our vocabulary \n",
        "        output_dim = 32, # Dimensions to which each words shall be mapped\n",
        "        input_length = MAX_SEQUENCE_LENGTH # Length of input sequences\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.Dropout(\n",
        "        rate=0.25 # Randomly disable 25% of neurons\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.LSTM(\n",
        "        units=32 # 32 LSTM units in this layer\n",
        "    )\n",
        ")\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.Dropout(\n",
        "        rate=0.25 # Randomly disable 25% of neurons\n",
        "    )\n",
        ")\n",
        "\n",
        "model.add(\n",
        "    tf.keras.layers.Dense(\n",
        "        units=1, # Single unit\n",
        "        activation='sigmoid' # Sigmoid activation function (output from 0 to 1)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.binary_crossentropy, # loss function\n",
        "    optimizer=tf.keras.optimizers.Adam(), # optimiser function\n",
        "    metrics=['accuracy']) # reporting metric\n",
        "\n",
        "# Display a summary of the models structure\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 374, 32)           22110752  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 374, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 22,119,105\n",
            "Trainable params: 22,119,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki4U2Z_zoJQk",
        "outputId": "62047895-8a11-4a1c-8086-3cadc51faebe"
      },
      "source": [
        "history = model.fit(x_train, y_train , batch_size=256, epochs=3,validation_split=0.2, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "3675/4000 [==========================>...] - ETA: 1:11 - loss: 0.3759 - accuracy: 0.8317"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_3mhHrToJQl"
      },
      "source": [
        "# Get Model Predictions for test data\n",
        "def lst_of_lst(lst):\n",
        "    return [[el] for el in lst]\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "predicted_classes = model.predict_classes(x_test)\n",
        "print(classification_report(lst_of_lst(y_test), predicted_classes, target_names=[0,1]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
