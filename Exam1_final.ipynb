{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exam1_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKDTJ10Hz0Fv"
      },
      "source": [
        "Panther ID: 002579345\r\n",
        "\r\n",
        "Name : Puthucode, Krishna Rukmini"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EbrP523-56Kb",
        "outputId": "f700223d-3626-4462-8b2c-306382a3291d"
      },
      "source": [
        "from zipfile import ZipFile\r\n",
        "file_name = \"/content/exam1_dataset.zip\"\r\n",
        "\r\n",
        "with ZipFile(file_name, 'r') as zip:\r\n",
        "  zip.extractall(\"/content\")\r\n",
        "  print('Done')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KGzatQVy7lin",
        "outputId": "54613912-50f9-4814-884b-2fa29587b34f"
      },
      "source": [
        "import os\r\n",
        "\r\n",
        "data_dir = \"/content/exam1_dataset\"\r\n",
        "files = os.listdir(data_dir+\"/\")\r\n",
        "files"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['UNLABELED', 'TRAINING', 'README']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Sey-Hypv731R",
        "outputId": "e1ad68e6-34fe-436f-90f9-637f1a556278"
      },
      "source": [
        "data_training_negative = []\r\n",
        "files_training = os.listdir(data_dir+\"/\"+files[1]+\"/\")\r\n",
        "files_training"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['negative', 'positive']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDGECa8s77Ss"
      },
      "source": [
        "def List_Of_Files(dir_Name): \r\n",
        "    listOfFile = [x for x in os.walk(dir_Name)]\r\n",
        "    return listOfFile   "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1jyhlPqJ8PnS",
        "outputId": "5c931236-ab45-413d-97d6-8b360c984f73"
      },
      "source": [
        "count = 0\r\n",
        "for file_train in files_training:\r\n",
        "  if count != 1 :\r\n",
        "    data_train_negative = List_Of_Files(\"/content/exam1_dataset/TRAINING/\" + file_train+\"/\")\r\n",
        "    count += 1\r\n",
        "  else:\r\n",
        "    print(file_train)\r\n",
        "    data_train_positive = List_Of_Files(\"/content/exam1_dataset/TRAINING/\" + file_train+\"/\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DzPX5Pep8tj-",
        "outputId": "5bb0fb33-ecc2-42fc-e36f-08a2c916d347"
      },
      "source": [
        "data_train_negative"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('/content/exam1_dataset/TRAINING/negative/',\n",
              "  [],\n",
              "  ['12268_1.txt',\n",
              "   '7015_4.txt',\n",
              "   '12026_1.txt',\n",
              "   '2061_1.txt',\n",
              "   '2024_1.txt',\n",
              "   '4094_2.txt',\n",
              "   '9701_1.txt',\n",
              "   '6236_1.txt',\n",
              "   '1983_1.txt',\n",
              "   '6973_3.txt',\n",
              "   '11374_2.txt',\n",
              "   '4934_4.txt',\n",
              "   '11392_1.txt',\n",
              "   '3957_1.txt',\n",
              "   '4457_4.txt',\n",
              "   '7535_1.txt',\n",
              "   '110_1.txt',\n",
              "   '6724_1.txt',\n",
              "   '2543_1.txt',\n",
              "   '7985_4.txt',\n",
              "   '6280_4.txt',\n",
              "   '141_3.txt',\n",
              "   '12397_4.txt',\n",
              "   '7302_3.txt',\n",
              "   '3095_1.txt',\n",
              "   '2690_2.txt',\n",
              "   '9476_4.txt',\n",
              "   '11950_2.txt',\n",
              "   '359_3.txt',\n",
              "   '4949_3.txt',\n",
              "   '10712_1.txt',\n",
              "   '5419_1.txt',\n",
              "   '1122_1.txt',\n",
              "   '5150_4.txt',\n",
              "   '2425_4.txt',\n",
              "   '7182_1.txt',\n",
              "   '6417_1.txt',\n",
              "   '4753_3.txt',\n",
              "   '5725_3.txt',\n",
              "   '6388_4.txt',\n",
              "   '7213_3.txt',\n",
              "   '7627_4.txt',\n",
              "   '7351_2.txt',\n",
              "   '6797_1.txt',\n",
              "   '11268_1.txt',\n",
              "   '1654_1.txt',\n",
              "   '1452_1.txt',\n",
              "   '1096_2.txt',\n",
              "   '8274_1.txt',\n",
              "   '1244_2.txt',\n",
              "   '7694_3.txt',\n",
              "   '5683_2.txt',\n",
              "   '4922_4.txt',\n",
              "   '5438_3.txt',\n",
              "   '11781_1.txt',\n",
              "   '9107_1.txt',\n",
              "   '7400_1.txt',\n",
              "   '8779_4.txt',\n",
              "   '3392_4.txt',\n",
              "   '10801_1.txt',\n",
              "   '7157_4.txt',\n",
              "   '4879_3.txt',\n",
              "   '10908_1.txt',\n",
              "   '10307_2.txt',\n",
              "   '1487_1.txt',\n",
              "   '7848_4.txt',\n",
              "   '10056_2.txt',\n",
              "   '10695_1.txt',\n",
              "   '2813_1.txt',\n",
              "   '2227_1.txt',\n",
              "   '11838_1.txt',\n",
              "   '5678_1.txt',\n",
              "   '75_1.txt',\n",
              "   '9312_1.txt',\n",
              "   '1722_1.txt',\n",
              "   '10147_1.txt',\n",
              "   '4785_2.txt',\n",
              "   '7593_2.txt',\n",
              "   '3805_2.txt',\n",
              "   '10648_4.txt',\n",
              "   '11857_1.txt',\n",
              "   '10180_1.txt',\n",
              "   '9316_3.txt',\n",
              "   '10115_1.txt',\n",
              "   '11539_1.txt',\n",
              "   '5966_3.txt',\n",
              "   '10710_1.txt',\n",
              "   '3517_1.txt',\n",
              "   '5073_1.txt',\n",
              "   '7055_3.txt',\n",
              "   '4515_1.txt',\n",
              "   '11557_1.txt',\n",
              "   '2871_1.txt',\n",
              "   '5169_2.txt',\n",
              "   '6542_1.txt',\n",
              "   '11846_2.txt',\n",
              "   '1488_1.txt',\n",
              "   '248_4.txt',\n",
              "   '5830_2.txt',\n",
              "   '4609_4.txt',\n",
              "   '9762_3.txt',\n",
              "   '9509_1.txt',\n",
              "   '6851_4.txt',\n",
              "   '2081_4.txt',\n",
              "   '960_1.txt',\n",
              "   '1001_4.txt',\n",
              "   '6789_4.txt',\n",
              "   '9063_2.txt',\n",
              "   '5415_1.txt',\n",
              "   '7173_1.txt',\n",
              "   '4670_4.txt',\n",
              "   '6056_2.txt',\n",
              "   '5408_3.txt',\n",
              "   '8784_3.txt',\n",
              "   '11707_1.txt',\n",
              "   '663_1.txt',\n",
              "   '10524_4.txt',\n",
              "   '12411_4.txt',\n",
              "   '1588_1.txt',\n",
              "   '6604_3.txt',\n",
              "   '4002_1.txt',\n",
              "   '3742_1.txt',\n",
              "   '2946_1.txt',\n",
              "   '2130_3.txt',\n",
              "   '2564_2.txt',\n",
              "   '2452_3.txt',\n",
              "   '10909_1.txt',\n",
              "   '10460_3.txt',\n",
              "   '7758_4.txt',\n",
              "   '783_1.txt',\n",
              "   '12284_4.txt',\n",
              "   '8803_4.txt',\n",
              "   '4073_1.txt',\n",
              "   '3333_4.txt',\n",
              "   '8471_1.txt',\n",
              "   '1360_4.txt',\n",
              "   '661_1.txt',\n",
              "   '7908_2.txt',\n",
              "   '2613_2.txt',\n",
              "   '1088_1.txt',\n",
              "   '6536_3.txt',\n",
              "   '7203_2.txt',\n",
              "   '9382_1.txt',\n",
              "   '3085_1.txt',\n",
              "   '1284_4.txt',\n",
              "   '2856_1.txt',\n",
              "   '8685_3.txt',\n",
              "   '10076_4.txt',\n",
              "   '7634_2.txt',\n",
              "   '11443_2.txt',\n",
              "   '7555_2.txt',\n",
              "   '4114_1.txt',\n",
              "   '4182_1.txt',\n",
              "   '6624_1.txt',\n",
              "   '2417_1.txt',\n",
              "   '77_4.txt',\n",
              "   '6449_2.txt',\n",
              "   '8598_1.txt',\n",
              "   '10839_2.txt',\n",
              "   '2979_4.txt',\n",
              "   '10171_3.txt',\n",
              "   '9088_4.txt',\n",
              "   '2665_1.txt',\n",
              "   '5044_1.txt',\n",
              "   '2500_1.txt',\n",
              "   '10685_3.txt',\n",
              "   '4033_1.txt',\n",
              "   '10991_1.txt',\n",
              "   '5108_1.txt',\n",
              "   '11786_4.txt',\n",
              "   '2848_3.txt',\n",
              "   '173_3.txt',\n",
              "   '8534_3.txt',\n",
              "   '508_3.txt',\n",
              "   '8156_4.txt',\n",
              "   '494_3.txt',\n",
              "   '9539_3.txt',\n",
              "   '3907_3.txt',\n",
              "   '8026_2.txt',\n",
              "   '8590_4.txt',\n",
              "   '301_1.txt',\n",
              "   '3760_1.txt',\n",
              "   '5480_4.txt',\n",
              "   '8745_2.txt',\n",
              "   '1985_1.txt',\n",
              "   '6036_1.txt',\n",
              "   '2050_2.txt',\n",
              "   '9828_3.txt',\n",
              "   '8652_1.txt',\n",
              "   '11444_2.txt',\n",
              "   '7828_1.txt',\n",
              "   '8726_4.txt',\n",
              "   '905_1.txt',\n",
              "   '5098_1.txt',\n",
              "   '3077_1.txt',\n",
              "   '4421_4.txt',\n",
              "   '1951_1.txt',\n",
              "   '9334_4.txt',\n",
              "   '5747_2.txt',\n",
              "   '787_4.txt',\n",
              "   '11852_1.txt',\n",
              "   '5997_3.txt',\n",
              "   '1401_3.txt',\n",
              "   '5037_4.txt',\n",
              "   '4695_4.txt',\n",
              "   '10579_2.txt',\n",
              "   '11240_4.txt',\n",
              "   '11188_1.txt',\n",
              "   '1832_4.txt',\n",
              "   '10165_3.txt',\n",
              "   '132_3.txt',\n",
              "   '5057_1.txt',\n",
              "   '8930_1.txt',\n",
              "   '9193_3.txt',\n",
              "   '6014_1.txt',\n",
              "   '2123_3.txt',\n",
              "   '10366_1.txt',\n",
              "   '2213_1.txt',\n",
              "   '619_1.txt',\n",
              "   '6549_1.txt',\n",
              "   '320_1.txt',\n",
              "   '813_3.txt',\n",
              "   '8985_2.txt',\n",
              "   '7631_4.txt',\n",
              "   '10990_1.txt',\n",
              "   '751_1.txt',\n",
              "   '4186_1.txt',\n",
              "   '5927_4.txt',\n",
              "   '1640_1.txt',\n",
              "   '2789_1.txt',\n",
              "   '7142_1.txt',\n",
              "   '12336_3.txt',\n",
              "   '3507_1.txt',\n",
              "   '5981_3.txt',\n",
              "   '10612_3.txt',\n",
              "   '6150_3.txt',\n",
              "   '10975_3.txt',\n",
              "   '7774_3.txt',\n",
              "   '8543_3.txt',\n",
              "   '10490_1.txt',\n",
              "   '8035_1.txt',\n",
              "   '7537_1.txt',\n",
              "   '4359_3.txt',\n",
              "   '5662_3.txt',\n",
              "   '2687_1.txt',\n",
              "   '945_2.txt',\n",
              "   '8953_1.txt',\n",
              "   '6710_1.txt',\n",
              "   '11638_4.txt',\n",
              "   '2222_4.txt',\n",
              "   '9666_1.txt',\n",
              "   '5655_1.txt',\n",
              "   '5690_2.txt',\n",
              "   '8767_1.txt',\n",
              "   '7485_4.txt',\n",
              "   '8359_4.txt',\n",
              "   '3371_4.txt',\n",
              "   '9179_3.txt',\n",
              "   '5281_1.txt',\n",
              "   '9505_4.txt',\n",
              "   '7791_1.txt',\n",
              "   '1692_1.txt',\n",
              "   '6695_1.txt',\n",
              "   '8236_1.txt',\n",
              "   '2065_2.txt',\n",
              "   '1320_1.txt',\n",
              "   '4742_1.txt',\n",
              "   '3692_1.txt',\n",
              "   '7545_1.txt',\n",
              "   '209_1.txt',\n",
              "   '5382_3.txt',\n",
              "   '11289_4.txt',\n",
              "   '10985_1.txt',\n",
              "   '7208_3.txt',\n",
              "   '5853_3.txt',\n",
              "   '10966_1.txt',\n",
              "   '8069_1.txt',\n",
              "   '1059_2.txt',\n",
              "   '8451_4.txt',\n",
              "   '11625_4.txt',\n",
              "   '1291_4.txt',\n",
              "   '7941_2.txt',\n",
              "   '7873_1.txt',\n",
              "   '2362_1.txt',\n",
              "   '465_1.txt',\n",
              "   '4466_2.txt',\n",
              "   '112_1.txt',\n",
              "   '10758_1.txt',\n",
              "   '6110_1.txt',\n",
              "   '4835_1.txt',\n",
              "   '2860_1.txt',\n",
              "   '11204_1.txt',\n",
              "   '11149_3.txt',\n",
              "   '8308_1.txt',\n",
              "   '12071_1.txt',\n",
              "   '9932_4.txt',\n",
              "   '2985_4.txt',\n",
              "   '3780_2.txt',\n",
              "   '11992_4.txt',\n",
              "   '7011_3.txt',\n",
              "   '11713_1.txt',\n",
              "   '2268_1.txt',\n",
              "   '6177_1.txt',\n",
              "   '5604_3.txt',\n",
              "   '11720_2.txt',\n",
              "   '2435_1.txt',\n",
              "   '8459_3.txt',\n",
              "   '1545_3.txt',\n",
              "   '9205_1.txt',\n",
              "   '66_4.txt',\n",
              "   '683_4.txt',\n",
              "   '3841_1.txt',\n",
              "   '1207_4.txt',\n",
              "   '8800_3.txt',\n",
              "   '6737_2.txt',\n",
              "   '1539_3.txt',\n",
              "   '3470_1.txt',\n",
              "   '10083_1.txt',\n",
              "   '4334_1.txt',\n",
              "   '511_1.txt',\n",
              "   '10846_1.txt',\n",
              "   '4136_1.txt',\n",
              "   '3219_2.txt',\n",
              "   '11036_1.txt',\n",
              "   '11436_1.txt',\n",
              "   '8198_4.txt',\n",
              "   '837_1.txt',\n",
              "   '3186_2.txt',\n",
              "   '719_4.txt',\n",
              "   '12337_1.txt',\n",
              "   '5855_1.txt',\n",
              "   '1478_4.txt',\n",
              "   '1769_4.txt',\n",
              "   '1763_1.txt',\n",
              "   '3701_2.txt',\n",
              "   '3651_4.txt',\n",
              "   '537_3.txt',\n",
              "   '1917_1.txt',\n",
              "   '5426_2.txt',\n",
              "   '3183_1.txt',\n",
              "   '3628_1.txt',\n",
              "   '8438_3.txt',\n",
              "   '3781_1.txt',\n",
              "   '11358_3.txt',\n",
              "   '6767_4.txt',\n",
              "   '4058_2.txt',\n",
              "   '8757_2.txt',\n",
              "   '3583_1.txt',\n",
              "   '7410_2.txt',\n",
              "   '757_3.txt',\n",
              "   '5249_1.txt',\n",
              "   '2376_4.txt',\n",
              "   '1690_1.txt',\n",
              "   '2895_4.txt',\n",
              "   '3940_1.txt',\n",
              "   '10440_1.txt',\n",
              "   '1788_3.txt',\n",
              "   '12149_4.txt',\n",
              "   '34_1.txt',\n",
              "   '6074_3.txt',\n",
              "   '4740_2.txt',\n",
              "   '5075_3.txt',\n",
              "   '4871_2.txt',\n",
              "   '8958_3.txt',\n",
              "   '6057_1.txt',\n",
              "   '10634_1.txt',\n",
              "   '7464_3.txt',\n",
              "   '6478_1.txt',\n",
              "   '12406_2.txt',\n",
              "   '8825_1.txt',\n",
              "   '519_1.txt',\n",
              "   '3225_3.txt',\n",
              "   '4090_3.txt',\n",
              "   '8563_2.txt',\n",
              "   '2020_2.txt',\n",
              "   '8203_3.txt',\n",
              "   '4861_1.txt',\n",
              "   '3081_2.txt',\n",
              "   '5711_3.txt',\n",
              "   '2617_1.txt',\n",
              "   '9971_1.txt',\n",
              "   '10371_3.txt',\n",
              "   '7739_4.txt',\n",
              "   '1691_4.txt',\n",
              "   '8213_4.txt',\n",
              "   '7966_4.txt',\n",
              "   '2164_2.txt',\n",
              "   '6064_3.txt',\n",
              "   '8974_4.txt',\n",
              "   '4877_2.txt',\n",
              "   '8664_1.txt',\n",
              "   '1569_2.txt',\n",
              "   '9567_1.txt',\n",
              "   '3269_4.txt',\n",
              "   '5053_2.txt',\n",
              "   '10445_1.txt',\n",
              "   '4104_4.txt',\n",
              "   '9898_1.txt',\n",
              "   '1335_4.txt',\n",
              "   '6433_3.txt',\n",
              "   '5884_1.txt',\n",
              "   '8618_1.txt',\n",
              "   '975_4.txt',\n",
              "   '6645_1.txt',\n",
              "   '8997_1.txt',\n",
              "   '3005_2.txt',\n",
              "   '3292_1.txt',\n",
              "   '10642_2.txt',\n",
              "   '10335_3.txt',\n",
              "   '1095_3.txt',\n",
              "   '5935_4.txt',\n",
              "   '4076_1.txt',\n",
              "   '3951_2.txt',\n",
              "   '3407_2.txt',\n",
              "   '10070_3.txt',\n",
              "   '9300_1.txt',\n",
              "   '5458_4.txt',\n",
              "   '9751_3.txt',\n",
              "   '3073_4.txt',\n",
              "   '7428_1.txt',\n",
              "   '1280_4.txt',\n",
              "   '2195_2.txt',\n",
              "   '10683_1.txt',\n",
              "   '5957_3.txt',\n",
              "   '9399_4.txt',\n",
              "   '3180_2.txt',\n",
              "   '6590_3.txt',\n",
              "   '631_3.txt',\n",
              "   '7696_2.txt',\n",
              "   '6696_4.txt',\n",
              "   '1465_3.txt',\n",
              "   '411_2.txt',\n",
              "   '1953_2.txt',\n",
              "   '4798_4.txt',\n",
              "   '1891_3.txt',\n",
              "   '10902_1.txt',\n",
              "   '6327_4.txt',\n",
              "   '6992_1.txt',\n",
              "   '6353_1.txt',\n",
              "   '6179_1.txt',\n",
              "   '5444_1.txt',\n",
              "   '7155_3.txt',\n",
              "   '1784_1.txt',\n",
              "   '1565_1.txt',\n",
              "   '8870_4.txt',\n",
              "   '8710_1.txt',\n",
              "   '11834_1.txt',\n",
              "   '406_2.txt',\n",
              "   '12121_4.txt',\n",
              "   '10238_4.txt',\n",
              "   '5679_1.txt',\n",
              "   '7610_3.txt',\n",
              "   '11271_3.txt',\n",
              "   '9649_1.txt',\n",
              "   '12303_1.txt',\n",
              "   '11255_1.txt',\n",
              "   '11378_1.txt',\n",
              "   '6344_2.txt',\n",
              "   '6566_4.txt',\n",
              "   '6377_1.txt',\n",
              "   '5651_1.txt',\n",
              "   '3345_1.txt',\n",
              "   '120_1.txt',\n",
              "   '829_1.txt',\n",
              "   '9764_1.txt',\n",
              "   '4596_2.txt',\n",
              "   '8444_1.txt',\n",
              "   '6364_1.txt',\n",
              "   '7656_1.txt',\n",
              "   '2341_4.txt',\n",
              "   '9653_2.txt',\n",
              "   '4581_1.txt',\n",
              "   '5425_4.txt',\n",
              "   '6846_1.txt',\n",
              "   '7703_3.txt',\n",
              "   '8406_3.txt',\n",
              "   '9082_1.txt',\n",
              "   '10764_3.txt',\n",
              "   '6697_2.txt',\n",
              "   '7313_1.txt',\n",
              "   '8788_3.txt',\n",
              "   '11970_4.txt',\n",
              "   '3189_1.txt',\n",
              "   '84_3.txt',\n",
              "   '2188_2.txt',\n",
              "   '7325_1.txt',\n",
              "   '2489_2.txt',\n",
              "   '10769_2.txt',\n",
              "   '6476_4.txt',\n",
              "   '6863_3.txt',\n",
              "   '1922_3.txt',\n",
              "   '2561_3.txt',\n",
              "   '10640_3.txt',\n",
              "   '4839_2.txt',\n",
              "   '1571_1.txt',\n",
              "   '6904_2.txt',\n",
              "   '8698_3.txt',\n",
              "   '9108_1.txt',\n",
              "   '7498_3.txt',\n",
              "   '8835_1.txt',\n",
              "   '2846_3.txt',\n",
              "   '12122_4.txt',\n",
              "   '160_2.txt',\n",
              "   '4946_1.txt',\n",
              "   '3463_1.txt',\n",
              "   '11853_1.txt',\n",
              "   '7118_3.txt',\n",
              "   '1761_1.txt',\n",
              "   '4372_2.txt',\n",
              "   '7744_1.txt',\n",
              "   '10963_1.txt',\n",
              "   '9407_4.txt',\n",
              "   '10734_4.txt',\n",
              "   '11463_1.txt',\n",
              "   '823_4.txt',\n",
              "   '7052_4.txt',\n",
              "   '10359_1.txt',\n",
              "   '12357_1.txt',\n",
              "   '9886_4.txt',\n",
              "   '4492_1.txt',\n",
              "   '12046_1.txt',\n",
              "   '7024_2.txt',\n",
              "   '9477_3.txt',\n",
              "   '9967_1.txt',\n",
              "   '6533_2.txt',\n",
              "   '3053_1.txt',\n",
              "   '3616_2.txt',\n",
              "   '11317_2.txt',\n",
              "   '4077_1.txt',\n",
              "   '11500_1.txt',\n",
              "   '10751_1.txt',\n",
              "   '8197_3.txt',\n",
              "   '10402_1.txt',\n",
              "   '2224_3.txt',\n",
              "   '11034_1.txt',\n",
              "   '12317_1.txt',\n",
              "   '8235_2.txt',\n",
              "   '11288_1.txt',\n",
              "   '10488_1.txt',\n",
              "   '12052_1.txt',\n",
              "   '7559_1.txt',\n",
              "   '9231_4.txt',\n",
              "   '9839_1.txt',\n",
              "   '608_3.txt',\n",
              "   '10775_3.txt',\n",
              "   '10071_1.txt',\n",
              "   '10058_1.txt',\n",
              "   '595_4.txt',\n",
              "   '10318_2.txt',\n",
              "   '765_4.txt',\n",
              "   '7390_2.txt',\n",
              "   '7981_4.txt',\n",
              "   '2570_3.txt',\n",
              "   '10957_3.txt',\n",
              "   '1811_1.txt',\n",
              "   '10559_3.txt',\n",
              "   '2717_1.txt',\n",
              "   '11055_1.txt',\n",
              "   '10245_3.txt',\n",
              "   '528_1.txt',\n",
              "   '3548_3.txt',\n",
              "   '12033_3.txt',\n",
              "   '8102_1.txt',\n",
              "   '1573_1.txt',\n",
              "   '1843_1.txt',\n",
              "   '3170_3.txt',\n",
              "   '9969_1.txt',\n",
              "   '3771_1.txt',\n",
              "   '11396_2.txt',\n",
              "   '1056_3.txt',\n",
              "   '9883_3.txt',\n",
              "   '933_2.txt',\n",
              "   '5990_2.txt',\n",
              "   '5608_1.txt',\n",
              "   '5463_1.txt',\n",
              "   '9775_4.txt',\n",
              "   '2009_4.txt',\n",
              "   '9138_1.txt',\n",
              "   '8264_4.txt',\n",
              "   '8397_3.txt',\n",
              "   '1450_3.txt',\n",
              "   '774_3.txt',\n",
              "   '7373_4.txt',\n",
              "   '69_4.txt',\n",
              "   '8812_3.txt',\n",
              "   '10357_1.txt',\n",
              "   '4569_1.txt',\n",
              "   '817_4.txt',\n",
              "   '1147_4.txt',\n",
              "   '10259_1.txt',\n",
              "   '4330_1.txt',\n",
              "   '1537_1.txt',\n",
              "   '7875_3.txt',\n",
              "   '5033_3.txt',\n",
              "   '1046_2.txt',\n",
              "   '9519_3.txt',\n",
              "   '11177_3.txt',\n",
              "   '625_4.txt',\n",
              "   '11676_2.txt',\n",
              "   '2741_2.txt',\n",
              "   '5906_1.txt',\n",
              "   '6824_4.txt',\n",
              "   '3373_1.txt',\n",
              "   '10216_3.txt',\n",
              "   '3395_4.txt',\n",
              "   '1814_1.txt',\n",
              "   '11297_3.txt',\n",
              "   '10812_1.txt',\n",
              "   '857_1.txt',\n",
              "   '10951_1.txt',\n",
              "   '4265_2.txt',\n",
              "   '7619_2.txt',\n",
              "   '10322_2.txt',\n",
              "   '11234_3.txt',\n",
              "   '8315_3.txt',\n",
              "   '5377_3.txt',\n",
              "   '705_2.txt',\n",
              "   '8495_3.txt',\n",
              "   '10633_1.txt',\n",
              "   '3557_4.txt',\n",
              "   '3672_4.txt',\n",
              "   '12069_4.txt',\n",
              "   '8992_4.txt',\n",
              "   '4860_2.txt',\n",
              "   '3106_1.txt',\n",
              "   '6391_2.txt',\n",
              "   '8365_4.txt',\n",
              "   '4601_4.txt',\n",
              "   '687_4.txt',\n",
              "   '10477_1.txt',\n",
              "   '11865_1.txt',\n",
              "   '3524_1.txt',\n",
              "   '4012_4.txt',\n",
              "   '6156_2.txt',\n",
              "   '9988_2.txt',\n",
              "   '1113_2.txt',\n",
              "   '1904_1.txt',\n",
              "   '11210_1.txt',\n",
              "   '4467_1.txt',\n",
              "   '2160_1.txt',\n",
              "   '3589_4.txt',\n",
              "   '12055_1.txt',\n",
              "   '7491_1.txt',\n",
              "   '6956_2.txt',\n",
              "   '7384_4.txt',\n",
              "   '7281_2.txt',\n",
              "   '943_2.txt',\n",
              "   '2032_2.txt',\n",
              "   '12038_4.txt',\n",
              "   '11020_4.txt',\n",
              "   '5786_2.txt',\n",
              "   '8207_3.txt',\n",
              "   '7711_3.txt',\n",
              "   '5730_1.txt',\n",
              "   '1234_2.txt',\n",
              "   '6639_1.txt',\n",
              "   '9570_2.txt',\n",
              "   '12176_2.txt',\n",
              "   '8814_4.txt',\n",
              "   '5200_1.txt',\n",
              "   '1618_3.txt',\n",
              "   '10376_1.txt',\n",
              "   '11291_1.txt',\n",
              "   '2652_1.txt',\n",
              "   '3973_3.txt',\n",
              "   '10729_1.txt',\n",
              "   '9020_1.txt',\n",
              "   '3716_3.txt',\n",
              "   '3970_2.txt',\n",
              "   '7931_3.txt',\n",
              "   '12276_2.txt',\n",
              "   '4313_4.txt',\n",
              "   '5637_2.txt',\n",
              "   '4666_2.txt',\n",
              "   '6035_1.txt',\n",
              "   '12356_1.txt',\n",
              "   '10886_4.txt',\n",
              "   '11748_1.txt',\n",
              "   '2228_1.txt',\n",
              "   '7465_1.txt',\n",
              "   '2151_1.txt',\n",
              "   '10392_3.txt',\n",
              "   '5790_1.txt',\n",
              "   '11822_4.txt',\n",
              "   '251_3.txt',\n",
              "   '12482_4.txt',\n",
              "   '8051_2.txt',\n",
              "   '8017_3.txt',\n",
              "   '2484_1.txt',\n",
              "   '6148_1.txt',\n",
              "   '11859_4.txt',\n",
              "   '8661_1.txt',\n",
              "   '9493_2.txt',\n",
              "   '6198_1.txt',\n",
              "   '9157_1.txt',\n",
              "   '45_2.txt',\n",
              "   '4801_4.txt',\n",
              "   '8378_1.txt',\n",
              "   '6199_1.txt',\n",
              "   '7915_4.txt',\n",
              "   '7595_1.txt',\n",
              "   '689_1.txt',\n",
              "   '9347_3.txt',\n",
              "   '2725_2.txt',\n",
              "   '7629_3.txt',\n",
              "   '1798_1.txt',\n",
              "   '7344_4.txt',\n",
              "   '11925_2.txt',\n",
              "   '9727_3.txt',\n",
              "   '9770_1.txt',\n",
              "   '9922_4.txt',\n",
              "   '2830_3.txt',\n",
              "   '8195_4.txt',\n",
              "   '10741_4.txt',\n",
              "   '4472_1.txt',\n",
              "   '12216_4.txt',\n",
              "   '9125_2.txt',\n",
              "   '5045_1.txt',\n",
              "   '3427_1.txt',\n",
              "   '4944_3.txt',\n",
              "   '6586_2.txt',\n",
              "   '3187_1.txt',\n",
              "   '8112_1.txt',\n",
              "   '12224_3.txt',\n",
              "   '7045_1.txt',\n",
              "   '1676_4.txt',\n",
              "   '8714_1.txt',\n",
              "   '5012_4.txt',\n",
              "   '7898_1.txt',\n",
              "   '4126_4.txt',\n",
              "   '435_2.txt',\n",
              "   '4040_1.txt',\n",
              "   '9725_2.txt',\n",
              "   '5030_3.txt',\n",
              "   '3184_2.txt',\n",
              "   '4461_2.txt',\n",
              "   '574_4.txt',\n",
              "   '9687_3.txt',\n",
              "   '7361_3.txt',\n",
              "   '993_2.txt',\n",
              "   '5595_1.txt',\n",
              "   '10930_1.txt',\n",
              "   '2033_1.txt',\n",
              "   '9582_4.txt',\n",
              "   '294_4.txt',\n",
              "   '8243_1.txt',\n",
              "   '3879_4.txt',\n",
              "   '4048_2.txt',\n",
              "   '5128_4.txt',\n",
              "   '9025_1.txt',\n",
              "   '6610_1.txt',\n",
              "   '3144_1.txt',\n",
              "   '4257_4.txt',\n",
              "   '680_1.txt',\n",
              "   '4100_3.txt',\n",
              "   '4103_2.txt',\n",
              "   '7935_4.txt',\n",
              "   '7723_4.txt',\n",
              "   '3254_3.txt',\n",
              "   '8925_3.txt',\n",
              "   '12124_1.txt',\n",
              "   '7001_1.txt',\n",
              "   '2619_4.txt',\n",
              "   '2454_4.txt',\n",
              "   '11632_1.txt',\n",
              "   '2219_1.txt',\n",
              "   '255_3.txt',\n",
              "   '3020_3.txt',\n",
              "   '4906_1.txt',\n",
              "   '3664_4.txt',\n",
              "   '9250_4.txt',\n",
              "   '8999_4.txt',\n",
              "   '11233_2.txt',\n",
              "   '6458_1.txt',\n",
              "   '3155_1.txt',\n",
              "   '6454_2.txt',\n",
              "   '194_1.txt',\n",
              "   '6938_4.txt',\n",
              "   '7888_1.txt',\n",
              "   '3582_3.txt',\n",
              "   '11647_2.txt',\n",
              "   '3745_2.txt',\n",
              "   '1189_4.txt',\n",
              "   '3991_4.txt',\n",
              "   '2624_1.txt',\n",
              "   '10100_1.txt',\n",
              "   '6030_1.txt',\n",
              "   '8610_2.txt',\n",
              "   '11785_1.txt',\n",
              "   '4762_1.txt',\n",
              "   '9501_2.txt',\n",
              "   '5290_3.txt',\n",
              "   '4068_3.txt',\n",
              "   '3715_2.txt',\n",
              "   '5004_3.txt',\n",
              "   '1731_3.txt',\n",
              "   '4557_2.txt',\n",
              "   '11404_4.txt',\n",
              "   '4711_1.txt',\n",
              "   '6934_4.txt',\n",
              "   '3038_4.txt',\n",
              "   '5590_1.txt',\n",
              "   '3696_4.txt',\n",
              "   '3586_4.txt',\n",
              "   '5582_3.txt',\n",
              "   '240_1.txt',\n",
              "   '9558_1.txt',\n",
              "   '4271_4.txt',\n",
              "   '2918_3.txt',\n",
              "   '461_1.txt',\n",
              "   '11778_3.txt',\n",
              "   '10364_4.txt',\n",
              "   '5814_3.txt',\n",
              "   '1389_2.txt',\n",
              "   '529_1.txt',\n",
              "   '2515_1.txt',\n",
              "   '7508_2.txt',\n",
              "   '2702_1.txt',\n",
              "   '3098_1.txt',\n",
              "   '10979_1.txt',\n",
              "   '7644_2.txt',\n",
              "   '9779_3.txt',\n",
              "   '652_2.txt',\n",
              "   '11457_4.txt',\n",
              "   '6427_3.txt',\n",
              "   '217_3.txt',\n",
              "   '10869_1.txt',\n",
              "   '5635_1.txt',\n",
              "   '391_4.txt',\n",
              "   '5407_1.txt',\n",
              "   '9893_4.txt',\n",
              "   '4214_1.txt',\n",
              "   '6926_4.txt',\n",
              "   '2581_1.txt',\n",
              "   '7356_1.txt',\n",
              "   '4247_4.txt',\n",
              "   '11559_4.txt',\n",
              "   '207_1.txt',\n",
              "   '1386_3.txt',\n",
              "   '1043_3.txt',\n",
              "   '2680_1.txt',\n",
              "   '8099_3.txt',\n",
              "   '2005_3.txt',\n",
              "   '2492_3.txt',\n",
              "   '4071_4.txt',\n",
              "   '7180_4.txt',\n",
              "   '5977_1.txt',\n",
              "   '9055_2.txt',\n",
              "   '11101_4.txt',\n",
              "   '10256_3.txt',\n",
              "   '941_2.txt',\n",
              "   '4661_3.txt',\n",
              "   '418_4.txt',\n",
              "   '2592_3.txt',\n",
              "   '7596_3.txt',\n",
              "   '9945_1.txt',\n",
              "   '11641_1.txt',\n",
              "   '4335_4.txt',\n",
              "   '840_1.txt',\n",
              "   '3660_1.txt',\n",
              "   '3806_4.txt',\n",
              "   '11894_3.txt',\n",
              "   '9503_4.txt',\n",
              "   '7462_1.txt',\n",
              "   '4394_1.txt',\n",
              "   '668_4.txt',\n",
              "   '8510_1.txt',\n",
              "   '7089_1.txt',\n",
              "   '632_1.txt',\n",
              "   '11070_1.txt',\n",
              "   '7686_1.txt',\n",
              "   '11287_3.txt',\n",
              "   '9767_3.txt',\n",
              "   '5948_4.txt',\n",
              "   '1962_2.txt',\n",
              "   '4127_1.txt',\n",
              "   '2982_1.txt',\n",
              "   '6269_1.txt',\n",
              "   '8246_3.txt',\n",
              "   '4446_3.txt',\n",
              "   '9273_1.txt',\n",
              "   '183_3.txt',\n",
              "   '284_2.txt',\n",
              "   '4679_4.txt',\n",
              "   '1321_1.txt',\n",
              "   '586_1.txt',\n",
              "   '7060_1.txt',\n",
              "   '613_3.txt',\n",
              "   '254_1.txt',\n",
              "   '8791_1.txt',\n",
              "   '10589_1.txt',\n",
              "   '3768_3.txt',\n",
              "   '9510_1.txt',\n",
              "   '3892_2.txt',\n",
              "   '4378_4.txt',\n",
              "   '81_1.txt',\n",
              "   '11201_1.txt',\n",
              "   '10323_1.txt',\n",
              "   '2768_2.txt',\n",
              "   '7592_3.txt',\n",
              "   '4691_2.txt',\n",
              "   '2899_1.txt',\n",
              "   '6003_2.txt',\n",
              "   '5465_1.txt',\n",
              "   '5816_3.txt',\n",
              "   '9445_1.txt',\n",
              "   '12496_1.txt',\n",
              "   '11703_1.txt',\n",
              "   '7476_4.txt',\n",
              "   '8480_3.txt',\n",
              "   '11219_3.txt',\n",
              "   '2974_1.txt',\n",
              "   '8626_1.txt',\n",
              "   '6422_1.txt',\n",
              "   '3107_3.txt',\n",
              "   '11329_1.txt',\n",
              "   '12217_4.txt',\n",
              "   '10006_4.txt',\n",
              "   '2883_1.txt',\n",
              "   '3240_1.txt',\n",
              "   '11496_4.txt',\n",
              "   '2393_2.txt',\n",
              "   '7144_2.txt',\n",
              "   '6052_2.txt',\n",
              "   '5138_1.txt',\n",
              "   '6483_1.txt',\n",
              "   '7622_4.txt',\n",
              "   '4353_4.txt',\n",
              "   '8604_3.txt',\n",
              "   '10370_4.txt',\n",
              "   '2088_1.txt',\n",
              "   '918_2.txt',\n",
              "   '9774_1.txt',\n",
              "   '7036_4.txt',\n",
              "   '7275_1.txt',\n",
              "   '10160_4.txt',\n",
              "   '4632_4.txt',\n",
              "   '3824_1.txt',\n",
              "   '11174_2.txt',\n",
              "   '5439_2.txt',\n",
              "   '12180_3.txt',\n",
              "   '8115_1.txt',\n",
              "   '12015_4.txt',\n",
              "   '11516_4.txt',\n",
              "   '4291_4.txt',\n",
              "   '8118_3.txt',\n",
              "   '4377_3.txt',\n",
              "   '7817_1.txt',\n",
              "   '11804_4.txt',\n",
              "   '9863_2.txt',\n",
              "   '12327_1.txt',\n",
              "   '12007_2.txt',\n",
              "   '3791_1.txt',\n",
              "   '4635_4.txt',\n",
              "   '8105_1.txt',\n",
              "   '6473_3.txt',\n",
              "   '10124_1.txt',\n",
              "   '7911_3.txt',\n",
              "   '6376_4.txt',\n",
              "   '7609_4.txt',\n",
              "   '10435_2.txt',\n",
              "   '1183_2.txt',\n",
              "   '903_3.txt',\n",
              "   '11856_1.txt',\n",
              "   '7263_4.txt',\n",
              "   '8859_3.txt',\n",
              "   '9853_1.txt',\n",
              "   '5271_4.txt',\n",
              "   '11947_4.txt',\n",
              "   '12174_4.txt',\n",
              "   '8976_1.txt',\n",
              "   '9718_4.txt',\n",
              "   '3426_3.txt',\n",
              "   '7668_1.txt',\n",
              "   '9208_1.txt',\n",
              "   '11670_1.txt',\n",
              "   '3457_4.txt',\n",
              "   '12465_2.txt',\n",
              "   '10575_3.txt',\n",
              "   '2857_1.txt',\n",
              "   '4447_2.txt',\n",
              "   '11043_1.txt',\n",
              "   '4496_4.txt',\n",
              "   '2423_4.txt',\n",
              "   '1341_3.txt',\n",
              "   '1509_2.txt',\n",
              "   '8914_1.txt',\n",
              "   '384_4.txt',\n",
              "   '2681_3.txt',\n",
              "   '7987_2.txt',\n",
              "   '1786_4.txt',\n",
              "   '3376_3.txt',\n",
              "   '7481_1.txt',\n",
              "   '12050_2.txt',\n",
              "   '2839_2.txt',\n",
              "   '8447_3.txt',\n",
              "   '211_4.txt',\n",
              "   '429_3.txt',\n",
              "   '7782_2.txt',\n",
              "   '12003_4.txt',\n",
              "   ...])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGDU25Gt8gBk"
      },
      "source": [
        "for d_t_n in data_train_negative[0][2]:\r\n",
        "  with open(data_train_negative[0][0]+\"/\"+d_t_n,'r') as fs:\r\n",
        "    data_training_negative.append(fs.read())\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjFuCOGe8jYt"
      },
      "source": [
        "data_training_positive = []\r\n",
        "for d_t_n in data_train_positive[0][2]:\r\n",
        "  with open(data_train_positive[0][0]+\"/\"+d_t_n,'r') as fs:\r\n",
        "    data_training_positive.append(fs.read())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpiX2zbB_LBa"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "positive_df = pd.DataFrame(data_training_positive,columns = ['Data'])\r\n",
        "positive_df['Label'] = \"Positive\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUYx9vzxAsEy"
      },
      "source": [
        "negative_df = pd.DataFrame(data_training_negative,columns = ['Data'])\r\n",
        "negative_df['Label'] = \"Negative\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4b6gSGzBP7g"
      },
      "source": [
        "data_df = pd.DataFrame()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "OqwQ4BpICM3e",
        "outputId": "da04c49e-0edc-4c40-c84e-d41919a44bd9"
      },
      "source": [
        "negative_df"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Data</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I've seen about four other Japanese horror fil...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"54\" is a film based on the infamous \"Studio 5...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I really wish that when making a comedy, the p...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The only film I've ever walked out on. Amazing...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Trying to cash in on the success of Deal Or No...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12495</th>\n",
              "      <td>This movie was definitely not one of Mary-Kate...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12496</th>\n",
              "      <td>Lil Bush is a 30 minute cartoon show comprised...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12497</th>\n",
              "      <td>Where was his critique of democratic administr...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12498</th>\n",
              "      <td>really awful... lead actor did OK... the film,...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12499</th>\n",
              "      <td>It is hard to make an unbiased judgment on a f...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12500 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Data     Label\n",
              "0      I've seen about four other Japanese horror fil...  Negative\n",
              "1      \"54\" is a film based on the infamous \"Studio 5...  Negative\n",
              "2      I really wish that when making a comedy, the p...  Negative\n",
              "3      The only film I've ever walked out on. Amazing...  Negative\n",
              "4      Trying to cash in on the success of Deal Or No...  Negative\n",
              "...                                                  ...       ...\n",
              "12495  This movie was definitely not one of Mary-Kate...  Negative\n",
              "12496  Lil Bush is a 30 minute cartoon show comprised...  Negative\n",
              "12497  Where was his critique of democratic administr...  Negative\n",
              "12498  really awful... lead actor did OK... the film,...  Negative\n",
              "12499  It is hard to make an unbiased judgment on a f...  Negative\n",
              "\n",
              "[12500 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFZClL-mBmr_"
      },
      "source": [
        "data_df = pd.concat([positive_df,negative_df])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "VUu3_HBUB62z",
        "outputId": "e2547110-60e9-4d3e-eecc-a79b9eceb4de"
      },
      "source": [
        "data_df"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Data</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bored with the normal, run-of-the-mill staple ...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I enjoyed this film, perhaps because I had not...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"Home Room\" like \"Zero Day\" and \"Elephant\", wa...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MPAA:Rated R for Violence,Language,Nudity and ...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>First, this is a review of the two disc set th...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12495</th>\n",
              "      <td>This movie was definitely not one of Mary-Kate...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12496</th>\n",
              "      <td>Lil Bush is a 30 minute cartoon show comprised...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12497</th>\n",
              "      <td>Where was his critique of democratic administr...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12498</th>\n",
              "      <td>really awful... lead actor did OK... the film,...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12499</th>\n",
              "      <td>It is hard to make an unbiased judgment on a f...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Data     Label\n",
              "0      Bored with the normal, run-of-the-mill staple ...  Positive\n",
              "1      I enjoyed this film, perhaps because I had not...  Positive\n",
              "2      \"Home Room\" like \"Zero Day\" and \"Elephant\", wa...  Positive\n",
              "3      MPAA:Rated R for Violence,Language,Nudity and ...  Positive\n",
              "4      First, this is a review of the two disc set th...  Positive\n",
              "...                                                  ...       ...\n",
              "12495  This movie was definitely not one of Mary-Kate...  Negative\n",
              "12496  Lil Bush is a 30 minute cartoon show comprised...  Negative\n",
              "12497  Where was his critique of democratic administr...  Negative\n",
              "12498  really awful... lead actor did OK... the film,...  Negative\n",
              "12499  It is hard to make an unbiased judgment on a f...  Negative\n",
              "\n",
              "[25000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1hZC2xOESuC"
      },
      "source": [
        "Question 1) (20 points) Write a generic function that takes: Classification algorithm name,\r\n",
        "vectorization method name, training set with labels as parameters (total of 3 parameters should\r\n",
        "be passed). The function should take the classification algorithm name, the vectorization\r\n",
        "methodâ€™s name, and the training set and train the desired model. Use the default training\r\n",
        "parameters for the models we have seen in class. This function should return the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH0CSvEiE1aY"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-nyLFkxEJaJ"
      },
      "source": [
        "def create_model(algorithm,vectorization,train_data):\r\n",
        "  model = make_pipeline(vectorization(),algorithm())\r\n",
        "  model = model.fit(train_data.Data,train_data.Label)\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pgabl35FSpi"
      },
      "source": [
        "Question 2) (30 points) Using the function from question 1 to build the following models:\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv5whfdrFYZ0"
      },
      "source": [
        "a) Model a: Naive Bayes, Vectorizer: TFIDF and Bag of Words, Training set should be 75%\r\n",
        "of the provided dataset. Leaving the remaining 25% for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4Aeo3QkFXn8"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj5nr_cbF7yk"
      },
      "source": [
        "train_Naive,test_Naive = train_test_split(data_df,test_size = 0.25,random_state = 12345 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYvfikXDHRab"
      },
      "source": [
        "model_Naive_TF = create_model(MultinomialNB,TfidfVectorizer,train_Naive)\r\n",
        "model_Naive = create_model(MultinomialNB,CountVectorizer,train_Naive)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOjqs-dVImwK"
      },
      "source": [
        "b) Model b: RandomForest, Vectorizer: TFIDF and Bag of Words, Training set should be\r\n",
        "70% of the provided dataset. Leaving the remaining 30% for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLXHf5qWIrTQ"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dB_2ieYHj8y"
      },
      "source": [
        "train_Random_Forest,test_Random_Forest = train_test_split(data_df,test_size = 0.30,random_state = 12345 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH1YlqyBIpkY"
      },
      "source": [
        "model_RFC_TF = create_model(RandomForestClassifier,TfidfVectorizer,train_Random_Forest)\r\n",
        "model_RFC = create_model(RandomForestClassifier,CountVectorizer,train_Random_Forest)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VMWtsExL2Bc"
      },
      "source": [
        "c) Model c:  (SVC in sklearn), Vectorizer: TFIDF and Bag of\r\n",
        "Words, Training set should be 60% of the provided dataset. Leaving the remaining 40%\r\n",
        "for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3sSPjW9LS2X"
      },
      "source": [
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fuE0sL9NjH2"
      },
      "source": [
        "train_SVC,test_SVC = train_test_split(data_df,test_size = 0.40,random_state = 12345 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NTPb06ROGRR"
      },
      "source": [
        "\r\n",
        "model_SVC = create_model(SVC,CountVectorizer,train_SVC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YVvrMb8Wdm6"
      },
      "source": [
        "model_SVC_TF = create_model(SVC,TfidfVectorizer,train_SVC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2LgC-Gszyf5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brQU7Ra5VVyf"
      },
      "source": [
        "\r\n",
        "label_Naive = model_Naive.predict(test_Naive.Data)\r\n",
        "label_RFC_TF = model_RFC_TF.predict(test_Random_Forest.Data)\r\n",
        "label_SVC = model_SVC.predict(test_SVC.Data)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leG5tQKfgTwi"
      },
      "source": [
        "label_SVC_TF = model_SVC_TF.predict(test_SVC.Data)\r\n",
        "label_Naive_TF = model_Naive_TF.predict(test_Naive.Data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5c6yEkMgjfx"
      },
      "source": [
        "label_RFC = model_RFC.predict(test_Random_Forest.Data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAEdazxqObbU"
      },
      "source": [
        "Question 3) (30 points) Using the models from Question 2, evaluate each model with its\r\n",
        "respective training set (for model a, that set is 25% of the data, for model b, that set is 30% of\r\n",
        "the data, and for model c that set is 40% of the data. Be careful to not mix up the evaluation\r\n",
        "sets. With the predictions on the test set and show the following metrics: Accuracy, Precision,\r\n",
        "Recall, and Macro F1-score. With this in mind, please write and answer these questions in your\r\n",
        "notebook:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQxJf_jHdAls"
      },
      "source": [
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p07_eSoCUfWl"
      },
      "source": [
        "F1_Scores = [sklearn.metrics.f1_score(label_Naive, test_Naive.Label , average='macro'), \r\n",
        " sklearn.metrics.f1_score(label_Naive_TF, test_Naive.Label , average='macro'),\r\n",
        " sklearn.metrics.f1_score(label_RFC, test_Random_Forest.Label, average='macro'), \r\n",
        " sklearn.metrics.f1_score(label_RFC_TF, test_Random_Forest.Label, average='macro'), \r\n",
        " sklearn.metrics.f1_score(label_SVC, test_SVC.Label, average='macro'),\r\n",
        " sklearn.metrics.f1_score(label_SVC_TF, test_SVC.Label, average='macro')]\r\n",
        "\r\n",
        "accuracy = [sklearn.metrics.accuracy_score(test_Naive.Label,label_Naive),\r\n",
        " sklearn.metrics.accuracy_score(test_Naive.Label,label_Naive_TF),\r\n",
        " sklearn.metrics.accuracy_score(test_Random_Forest.Label,label_RFC),\r\n",
        " sklearn.metrics.accuracy_score(test_Random_Forest.Label,label_RFC_TF),\r\n",
        " sklearn.metrics.accuracy_score(test_SVC.Label,label_SVC),\r\n",
        " sklearn.metrics.accuracy_score(test_SVC.Label,label_SVC_TF)]\r\n",
        "\r\n",
        "Precision = [sklearn.metrics.precision_score(test_Naive.Label,label_Naive, pos_label = \"Positive\"),\r\n",
        " sklearn.metrics.precision_score(test_Naive.Label,label_Naive_TF,pos_label = \"Positive\"),\r\n",
        " sklearn.metrics.precision_score(test_Random_Forest.Label,label_RFC,pos_label = \"Positive\"),\r\n",
        " sklearn.metrics.precision_score(test_Random_Forest.Label,label_RFC_TF,pos_label = \"Positive\"),\r\n",
        " sklearn.metrics.precision_score(test_SVC.Label,label_SVC,pos_label = \"Positive\"),\r\n",
        " sklearn.metrics.precision_score(test_SVC.Label,label_SVC_TF,pos_label = \"Positive\")]\r\n",
        "\r\n",
        "Recall = [sklearn.metrics.recall_score(test_Naive.Label,label_Naive,pos_label = \"Positive\"),\r\n",
        " sklearn.metrics.recall_score(test_Naive.Label,label_Naive_TF,pos_label = \"Positive\"),\r\n",
        " sklearn.metrics.recall_score(test_Random_Forest.Label,label_RFC,pos_label = \"Positive\"),\r\n",
        " sklearn.metrics.recall_score(test_Random_Forest.Label,label_RFC_TF,pos_label = \"Positive\"),\r\n",
        " sklearn.metrics.recall_score(test_SVC.Label,label_SVC,pos_label = \"Positive\"),\r\n",
        " sklearn.metrics.recall_score(test_SVC.Label,label_SVC_TF,pos_label = \"Positive\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLI-Ekewf2Gq"
      },
      "source": [
        "model_name = ['model_Naive','model_Naive_TF','model_RFC','model_RFC_TF','model_SVC','model_SVC_TF']\r\n",
        "training_prec = [75,75,70,70,60,60]\r\n",
        "testing_perc = [25,25,30,30,40,40]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAApYmK4hGva"
      },
      "source": [
        "performance = pd.DataFrame()\r\n",
        "performance['Model Name'] = model_name\r\n",
        "performance['Training percent'] = training_prec\r\n",
        "performance['Testing percent'] = testing_perc\r\n",
        "performance['Accuracy'] = accuracy\r\n",
        "performance['Precision'] = Precision\r\n",
        "performance['Recall'] = Recall\r\n",
        "performance['Macro_F1-Score'] = F1_Scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "hhHF-xsBc98t",
        "outputId": "6ad642bb-6cad-42df-86ce-d140d002c396"
      },
      "source": [
        "performance"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model Name</th>\n",
              "      <th>Training percent</th>\n",
              "      <th>Testing percent</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Macro_F1-Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>model_Naive</td>\n",
              "      <td>75</td>\n",
              "      <td>25</td>\n",
              "      <td>0.851360</td>\n",
              "      <td>0.882333</td>\n",
              "      <td>0.818580</td>\n",
              "      <td>0.851331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>model_Naive_TF</td>\n",
              "      <td>75</td>\n",
              "      <td>25</td>\n",
              "      <td>0.862080</td>\n",
              "      <td>0.895093</td>\n",
              "      <td>0.827338</td>\n",
              "      <td>0.862046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>model_RFC</td>\n",
              "      <td>70</td>\n",
              "      <td>30</td>\n",
              "      <td>0.840400</td>\n",
              "      <td>0.846925</td>\n",
              "      <td>0.838735</td>\n",
              "      <td>0.840362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>model_RFC_TF</td>\n",
              "      <td>70</td>\n",
              "      <td>30</td>\n",
              "      <td>0.835067</td>\n",
              "      <td>0.848452</td>\n",
              "      <td>0.823837</td>\n",
              "      <td>0.835062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>model_SVC</td>\n",
              "      <td>60</td>\n",
              "      <td>40</td>\n",
              "      <td>0.853800</td>\n",
              "      <td>0.842578</td>\n",
              "      <td>0.873315</td>\n",
              "      <td>0.853692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>model_SVC_TF</td>\n",
              "      <td>60</td>\n",
              "      <td>40</td>\n",
              "      <td>0.892200</td>\n",
              "      <td>0.883411</td>\n",
              "      <td>0.905829</td>\n",
              "      <td>0.892150</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Model Name  Training percent  ...    Recall  Macro_F1-Score\n",
              "0     model_Naive                75  ...  0.818580        0.851331\n",
              "1  model_Naive_TF                75  ...  0.827338        0.862046\n",
              "2       model_RFC                70  ...  0.838735        0.840362\n",
              "3    model_RFC_TF                70  ...  0.823837        0.835062\n",
              "4       model_SVC                60  ...  0.873315        0.853692\n",
              "5    model_SVC_TF                60  ...  0.905829        0.892150\n",
              "\n",
              "[6 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NsOvtOwea2Y"
      },
      "source": [
        "def Best_Performing_Model(performance_df):\r\n",
        "  Best_Model = performance_df.sort_values(by=['Accuracy'], ascending=False,inplace= False,axis = 0).head(1)['Model Name'].values[0]\r\n",
        "  return Best_Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "b_ITP9qYfamQ",
        "outputId": "980a9670-0206-4088-b868-ac9dc1ed038c"
      },
      "source": [
        "best_model = Best_Performing_Model(performance)\r\n",
        "best_model\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'model_SVC_TF'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-3NtLhhl8bM"
      },
      "source": [
        "a) What model performs the best and why? (which metrics do you base this on, and why do\r\n",
        "you think it performs better than others).\r\n",
        "\r\n",
        "Best Model is SVM when used TFIDF vectorization, because it's Accuracy is high and also the traning set considered is the highest(i.e., 75 %)\r\n",
        "\r\n",
        "here I'm considering Accuracy as both positive and negative classes are balanced.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCIMX2ObpEMz"
      },
      "source": [
        "b) Why is it important not to mix up the testing sets between different models? Think about\r\n",
        "this one.\r\n",
        "\r\n",
        "Test data represents the data set as a whole, so we don't pick a test set with different characteristics than the training set.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6QCyZ7HqBPd"
      },
      "source": [
        "c) Display in a single sorted dataframe (model name, training %, test %, accuracy,\r\n",
        "precision, recall, F1-score) all performance metrics, sorted by accuracy in descending\r\n",
        "manner.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "y62F3V2nfcrw",
        "outputId": "c89bff20-7b1a-4c45-d118-89b6f0b5feb1"
      },
      "source": [
        "performance.sort_values(by=['Accuracy'], ascending=False,inplace= False,axis = 0)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model Name</th>\n",
              "      <th>Training percent</th>\n",
              "      <th>Testing percent</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Macro_F1-Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>model_SVC_TF</td>\n",
              "      <td>60</td>\n",
              "      <td>40</td>\n",
              "      <td>0.892200</td>\n",
              "      <td>0.883411</td>\n",
              "      <td>0.905829</td>\n",
              "      <td>0.892150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>model_Naive_TF</td>\n",
              "      <td>75</td>\n",
              "      <td>25</td>\n",
              "      <td>0.862080</td>\n",
              "      <td>0.895093</td>\n",
              "      <td>0.827338</td>\n",
              "      <td>0.862046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>model_SVC</td>\n",
              "      <td>60</td>\n",
              "      <td>40</td>\n",
              "      <td>0.853800</td>\n",
              "      <td>0.842578</td>\n",
              "      <td>0.873315</td>\n",
              "      <td>0.853692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>model_Naive</td>\n",
              "      <td>75</td>\n",
              "      <td>25</td>\n",
              "      <td>0.851360</td>\n",
              "      <td>0.882333</td>\n",
              "      <td>0.818580</td>\n",
              "      <td>0.851331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>model_RFC</td>\n",
              "      <td>70</td>\n",
              "      <td>30</td>\n",
              "      <td>0.840400</td>\n",
              "      <td>0.846925</td>\n",
              "      <td>0.838735</td>\n",
              "      <td>0.840362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>model_RFC_TF</td>\n",
              "      <td>70</td>\n",
              "      <td>30</td>\n",
              "      <td>0.835067</td>\n",
              "      <td>0.848452</td>\n",
              "      <td>0.823837</td>\n",
              "      <td>0.835062</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Model Name  Training percent  ...    Recall  Macro_F1-Score\n",
              "5    model_SVC_TF                60  ...  0.905829        0.892150\n",
              "1  model_Naive_TF                75  ...  0.827338        0.862046\n",
              "4       model_SVC                60  ...  0.873315        0.853692\n",
              "0     model_Naive                75  ...  0.818580        0.851331\n",
              "2       model_RFC                70  ...  0.838735        0.840362\n",
              "3    model_RFC_TF                70  ...  0.823837        0.835062\n",
              "\n",
              "[6 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plkoMno4qNiR"
      },
      "source": [
        "Question 4) (15 points) Using the documents in the folder named UNLABELED, please use\r\n",
        "your best performing trained model from question 3 to predict their labels. Please do this\r\n",
        "individually for each document. Print to the screen the following items: Document Name,\r\n",
        "Predicted Label and using a text cell, write your own opinion if the label is correct and why -\r\n",
        "note you have to read the document to make your own opinion.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKUJzPP1fhDj",
        "outputId": "bc939e3f-fea9-4c7b-b730-ab3697169bac"
      },
      "source": [
        "Files_unlabeled = os.listdir(data_dir+\"/\")\r\n",
        "Files_unlabeled"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['UNLABELED', 'TRAINING', 'README']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6pPr6IerK9K"
      },
      "source": [
        "List_unlabeled_files = List_Of_Files(\"/content/exam1_dataset/\" + Files_unlabeled[0]+\"/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7NPqKA7rrx7",
        "outputId": "8ac36fe5-19f1-4409-d0ce-fcb93df11cfe"
      },
      "source": [
        "len(List_unlabeled_files[0][2])\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX_TcAg2tDGo"
      },
      "source": [
        "\r\n",
        "def output_print(model,txt,dir):\r\n",
        "  lt = []\r\n",
        "  #print(txt)\r\n",
        "  with open(dir+txt,'r') as fs:\r\n",
        "    #print(dir+txt)\r\n",
        "    lt.append(fs.read()) \r\n",
        "    label = model.predict([lt[0]])\r\n",
        "    \r\n",
        "    print(\"Document Name: \" ,txt)\r\n",
        "    print(\"Document content: \", lt[0])\r\n",
        "    print(\"Label pedicted: \" ,label[0])\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N8yGI5w0V1P",
        "outputId": "c9e4db70-ba8a-42da-bccf-e6a93f1f4d5f"
      },
      "source": [
        "output_print(model_SVC_TF,List_unlabeled_files[0][2][0],\"/content/exam1_dataset/UNLABELED/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document Name:  46278_0.txt\n",
            "Document content:  I was in this movie. Nothing big, just ran out of a van and up some steps and that's it. A friend of mine was in a few scenes too (trips up the main character during class I think). I remember being pulled out of class and sent down to an area where many kids were lined up to see if they'd get a part. They filmed, what I guess was the city public school scenes, at my school; Alexandria Primary School in Sydney.<br /><br />I can't remember much of the movie. I think a number of us went to see a screening of it over in North Sydney. We had an introduction from the cast & crew but that's about all I remember. Oh, and the \"food & drink\" they put on for us consisted of a paper cup of candy popcorn & a cup of lemonade. Ritzy! Still, it got me out of quite of bit of school that day, earned me $50 and each take of my scene (and there were a few) I had to eat an Ã©clair. Not bad.\n",
            "Label pedicted:  Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Rexo4twy3eY"
      },
      "source": [
        "opinion: It has many negative words in it thus I consider it to be a negative review.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOaY1ZJQ04Se",
        "outputId": "cca55c25-35f8-4a1b-8635-51344a32a814"
      },
      "source": [
        "output_print(model_SVC_TF,List_unlabeled_files[0][2][1],\"/content/exam1_dataset/UNLABELED/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document Name:  36517_0.txt\n",
            "Document content:  (Someone needs to say it)<br /><br />Possibly the worst, most depressingly dreary and poorly lit movie ever made, I find it unbelievable that anyone could find any merit in this turd. Bad, bad, bad... even the gore is bad. I usually am a big fan of the red stuff, but here is was just cheesy and nasty. They didn't even try. Far too long and boring to be funny either. Barely 6 people die in the course of the movie, the zombies look terrible (Gino De Rossi went overboard on the maggots), the kid was an obnoxious freak, and the music caused my ears to bleed! Only see this if you are on the brink of suicide and need just a little push to go all the way through with it. It'll make you feel ashamed of being a fan of Italian movies or Horror flicks. Burn this movie.\n",
            "Label pedicted:  Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z0NDxWw77rc"
      },
      "source": [
        "opinion: I think it's correctly predicted as it has many negative words clearly mentioned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMU2y7TL1z6Z",
        "outputId": "e99a61f9-c7f6-4c36-cb15-039fda7dbf37"
      },
      "source": [
        "output_print(model_SVC_TF,List_unlabeled_files[0][2][2],\"/content/exam1_dataset/UNLABELED/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document Name:  24221_0.txt\n",
            "Document content:  I don't know why but I used to be a huge fan of Creepshow and Tales from the crypt although the later series were rather lame. \"Trapped Ashes\" is very much in this tradition and features 4 horror shorts bound together in a surrounding plot of 2 couples and 2 solo people going on a sightseeing on a Hollywood movie site and getting stuck in an hold horror house with an old guy who leads them around. Turns out they can only leave if each tells their most frightening personal story... so here we go... Story 1 is about an actress who doesn't get any new jobs and decides to get a boob job. Now after her life turns around for the positive she soon realizes that her breast are vampire boobies feeding on human blood which doesn't go to well for her lovers. This story is pretty bizarre and trashy and captures the spirit of old \"Tales from the crypt\" stories best. Its totally idiotic but the end is so over the top its fun. Story 2 is a ghost-story about a couple moving to Japan where the woman is seduced by a monk who dies and takes her to hell where she turns into a succubus. Her husband soon learns he has to free her from there by feeding her a spell. The story is OK, but as frightening or thrilling as a Sesamestreet Episode. At least you get some naked shots and some nice animation sequences of old Japanese paintings which work pretty well. Story 3 is pretty much nothing leading nowhere. An actor and his best friend a scriptwriter regularly meet for chess until a girl appears. Soon the scriptwriter disappears and the actor begins a love affair to find out years later that the girl is some kind of ghost/witch/vampire... honestly I couldn't care less because the episode is boring and makes no sense. Story 4 is another strange one about a girl who grows up in her mothers womb along with a tapeworm and lives with a strange desire for collecting food for her \"twin\". When she is treated badly by her stepmother the \"twin\" takes revenge.<br /><br />What most of the stories suffer from is incredibly long passages of introducing of characters and that is way stretched and often even unnecessary for the plod. So when the action starts most of the time is up and there is not too much time for the horror to happen. Like most of those story collections there is some bad apples in there and I couldn't recommend this average movie just for the vampire boobies and the finale. This is just for real die hard fans of horror shorts... the slow ghost movies won't be too interesting for neither \"Tales...\" Fans nor others because they don't lead nowhere. Too bad...\n",
            "Label pedicted:  Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdtFj8li0XGF"
      },
      "source": [
        "opinion:  Seems like the reviwer had mixed opinions on the movie, thus start away determining which side it falls in is unclear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkBjNclU6vkq",
        "outputId": "e4e7ab97-9efb-4d85-ae81-340ae147d4d0"
      },
      "source": [
        "output_print(model_SVC_TF,List_unlabeled_files[0][2][3],\"/content/exam1_dataset/UNLABELED/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document Name:  46705_0.txt\n",
            "Document content:  Synopsis: Clare (Jackey Hall) and Julie (Andrea Ownbey), the campus snobs of Arkham University, are attempting to seek their own brand of revenge on goth girl Sarah (Ciara Richards) and her friend Allison (Adrianna Eder). When the campus is overrun by zombies, the snobs see a good opportunity to get back at their black-clad enemies. Unfortunately, things may not go as smoothly as Clare & Julie want and they might be the zombies' prey instead. . .<br /><br />Review: This is about as bad as a zombie flick can get these days. Other than the attractive females that like to take their clothes off, there isn't a single positive element to this stinkfest. It's not funny, it's not scary, it's just horrible. The story, if there is one, makes little sense at all. The script (if there actually was one and it wasn't just a bunch of idiots running around speaking in 90s clichÃ©s) is offensively bad, and it's only worsened by sincerely THE worst acting I've ever seen in my life (no exaggeration). I mean, these people make walking look difficult. They make Keanu Reeves look like Laurence Olivier. As the film progresses to the actual 'zombie invasion' (which, strangely, wasn't the first ten minutes when zombies were wandering all over campus), it only gets worse and worse. The direction during 'action' (for lack of a better word) scenes is almost unwatchable. Hell, even during the still shots, the camera-work is pathetic. By the end, it's a grueling experience to get through and I, with my very high tolerance for total crap, barely made it through. This is, without a doubt, one of the worst films I have ever seen and will probably ever see. Avoid at all costs.<br /><br />Obligatory Zombie Elements:<br /><br />Cause of Outbreak: The spread of blood of a zombie brought back to the states by some idiot professor. . . or something.<br /><br />Zombie Characteristics: They're mostly slow, shambling, etc., except for the occasional one with superhuman strength and kung fu abilities.<br /><br />Zombie Effects: Apparently powdered sugar and ketchup = Zombie faces. Not even remotely terrifying or realistic.<br /><br />Violence/Gore: There may be quite a bit of zombie violence, but the gore is just ridiculously bad and some of the worst I've ever seen. If strawberry syrup and Fruit Roll-Ups are disgusting gore to you, then this is the film for you.<br /><br />Sex/Nudity: The film spends the first ten or so minutes acting like softcore porn, but that ends abruptly and the rest of the film is spent mostly clothed.<br /><br />- - - <br /><br />Final Verdict: 1/10. Truly, truly painful.<br /><br />Recommended? I would not wish this curse upon anyone.<br /><br />-AP3-\n",
            "Label pedicted:  Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In0fQjDS1Ff4"
      },
      "source": [
        "opinion: I feel that the review is negative as it is clearly mentioned by the reviewer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdTHcs2l8j5h",
        "outputId": "aa5a4f11-2c91-435c-a4ae-ae669080ee3c"
      },
      "source": [
        "output_print(model_SVC_TF,List_unlabeled_files[0][2][4],\"/content/exam1_dataset/UNLABELED/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document Name:  37154_0.txt\n",
            "Document content:  ...and believe me, a 3 is very generous and based solely on my affinity for all things dinosaurian. First of all, these \"raptors\" look pretty lame. They are hardly raptorian, in the strict Velocirpator sense. I don't see any of the Velociraptors' infamous toe-claws on any of these things. They look like your typical, run-of-the-mill small predatory dinosaurs that would be more likely to attack small lizards and birds than human beings. To be honest, the Navy SEALs really should be able to beat these raptors senseless. Or just shoot them once and kill them. Lorenzo Lamas gives a typically bad Lorenzo Lamas performance. Not only is he a really bad Navy SEAL, he's a really bad actor. The girl Hayley Du Mond is attractive and keeps your interest for awhile, there's some sort of Allosaurus-type beast that appears from time to time to lessen our boredom. Overall, this is a pretty bad movie.\n",
            "Label pedicted:  Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbAXaKsj1d1E"
      },
      "source": [
        "Opinion: According to the review given it can be classified as a negative review as it shows that the reviewer was very badly disappointed by the Movie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnFlYrYK8mZ0",
        "outputId": "b3314f31-3be7-421d-d233-80c9e630346f"
      },
      "source": [
        "output_print(model_SVC_TF,List_unlabeled_files[0][2][5],\"/content/exam1_dataset/UNLABELED/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document Name:  0_0.txt\n",
            "Document content:  I admit, the great majority of films released before say 1933 are just not for me. Of the dozen or so \"major\" silents I have viewed, one I loved (The Crowd), and two were very good (The Last Command and City Lights, that latter Chaplin circa 1931).<br /><br />So I was apprehensive about this one, and humor is often difficult to appreciate (uh, enjoy) decades later. I did like the lead actors, but thought little of the film.<br /><br />One intriguing sequence. Early on, the guys are supposed to get \"de-loused\" and for about three minutes, fully dressed, do some schtick. In the background, perhaps three dozen men pass by, all naked, white and black (WWI ?), and for most, their butts, part or full backside, are shown. Was this an early variation of beefcake courtesy of Howard Hughes?\n",
            "Label pedicted:  Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niphpWr71_IL"
      },
      "source": [
        "Opinion: The review has many positive words mentioned thus I would consider it as a positive review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSvmF2pe8tua",
        "outputId": "685abf4d-3914-4f90-f32d-73640db1da0f"
      },
      "source": [
        "output_print(model_SVC_TF,List_unlabeled_files[0][2][6],\"/content/exam1_dataset/UNLABELED/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document Name:  36022_0.txt\n",
            "Document content:  Marie Windsor is brain washed by moon women. Some scenes are in early 3D.<br /><br />Windsor had much better showcase in The Girl in Black Stockings with yummy Mamie van Doren and delicious Lex Barker.\n",
            "Label pedicted:  Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICxa40vh2UWU"
      },
      "source": [
        "Opinion: Here the reviewer is comparing with another play and says that this particular one is not up to the mark, thus can be considered as a negative review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cU_Cd6tV8v5z",
        "outputId": "c8a9da5f-d55a-4493-e864-001c612ae282"
      },
      "source": [
        "output_print(model_SVC_TF,List_unlabeled_files[0][2][7],\"/content/exam1_dataset/UNLABELED/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document Name:  36149_0.txt\n",
            "Document content:  The first one was bad enough, and the short skirts, one take filming, and Jack Palance in funny hats just made me nauseous.\n",
            "Label pedicted:  Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPd7MbbH2n6o"
      },
      "source": [
        "opinion: It is clearly mentioned tht it's a bad movie thus can be considered as a negative review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RajWj5HG8yjq",
        "outputId": "6a253dc4-b5ab-4286-b5c3-21916b4209e0"
      },
      "source": [
        "output_print(model_SVC_TF,List_unlabeled_files[0][2][8],\"/content/exam1_dataset/UNLABELED/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document Name:  35991_0.txt\n",
            "Document content:  Just finished watching this and its a pretty bad film. A poor storyline, a rubbish script, and another low budget film where everything goes well for the good, and the bad get punished or learn a \"valuable\" lesson (even though really it was the leading female characters fault for closing down the research in the first place). The leading female character is always right and seems to be an expert in every field telling people what do (another low budget trait). This film just regurgitates all the bad things about low budget disaster movies. Its a shame because if this had been directed and scripted in more of a unique way, it could have brought back feelings of the old B movies about bugs etc.\n",
            "Label pedicted:  Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAY0x-MM224i"
      },
      "source": [
        "opinion: the words used in the review represents the reviewer didn't enjoy the movie thus can be considered as a negative review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hwUD_qR80pH",
        "outputId": "e2ff2b2f-5015-4b8d-8097-0876f956d712"
      },
      "source": [
        "output_print(model_SVC_TF,List_unlabeled_files[0][2][9],\"/content/exam1_dataset/UNLABELED/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document Name:  49990_0.txt\n",
            "Document content:  A group of five friends travel to Ireland during their school break vacation in order to do some shrooms in the wilderness along with their Irish friend/ mushroom guide. When Tara (Lindsey Haun) takes a particularly bad mushroom, she fills her mind with horrific visions of the past and of her friends' future. A future that will bring death and destruction. It's also a future that will hold boredom, and a wasted hour and a half for those who choose to watch this uninvolving, predictable, agonizingly slow horror flick. The acting is all right, but the scares are just sadly non-existent. Also the ending (which anyone with half a brain can figure out EARLY in the movie) is pretty much a direct rip-off of another modern horror film (no spoilers) <br /><br />My Grade: D+\n",
            "Label pedicted:  Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwsVDdzL3PlO"
      },
      "source": [
        "opinion  : the reviewer has given a D+ rating for the movie which shows that it's a negative review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBCMj65985VS",
        "outputId": "e4dbef2a-e18e-43d3-85b5-20ff526e3940"
      },
      "source": [
        "output_print(model_SVC_TF,List_unlabeled_files[0][2][10],\"/content/exam1_dataset/UNLABELED/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document Name:  35968_0.txt\n",
            "Document content:  You can't blame Blake Edwards for making this kind of movie.<br /><br />For years, he depended on the kind of pratfalls that course through \"A Fine Mess\" as his bread and butter, so to speak. They served the \"Pink Panther\" series well, and made Inspector Clouseau a world-wide reference point for the ultimate in clumsiness.<br /><br />But for a movie that basically features two losers crossing the mob in a horse race then moving a piano to a rich lady's house, this film is all over the place. So many people introduced then forgotten, plot lines that go nowhere, laughs that are fun for the moment but have no context.<br /><br />Shocking, really, this coming as it does from Blake Edwards, who once personified classy comedy with such works as the aforementioned \"Panther\" films, not to mention classics like \"10\", \"Micki + Maude\" and the under-appreciated \"S.O.B.\".<br /><br />And with the calibre of talent, you'd expect great things; the manic Mandel, lecherous Danson, luxuriant Alonzo, and wackos like Mulligan and Margolin as mob flunkies all have the fire, but there's just nothing here to stoke the furnace.<br /><br />There were separate moments here and there that gave me a smile but, like the movie itself, it just lives for the moment, then is gone.<br /><br />TIDBIT - The idea for this movie actually came from a Laurel and Hardy short where Stan and Ollie try to move a huge piano up innumerable flights of stairs. Hence, the name.<br /><br />It still is fitting: this movie is definitely a \"Mess\", if not a \"Fine\" one.<br /><br />Three stars. Saved but for the virtue of Mulligan in the cast and a bit part for pre-\"NYPD Blue\" Franz.\n",
            "Label pedicted:  Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "golKYx543hlL"
      },
      "source": [
        "opinion: the movie ws given a three stars by the reviewer which seems to be a positive review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FowDNPFKA25e"
      },
      "source": [
        "Question 7) (20 points) Using Spacyâ€™s part of speech tagger, process all sentences (hint: donâ€™t\r\n",
        "forget to split the reviews) and count how many NOUN and VERB tags are found in all the\r\n",
        "movies review (TRAINING folder) separating them by label. In other words, how many NOUN\r\n",
        "and VERB tags are found in positive reviews, and how many NOUN and VERB tags are found\r\n",
        "in negative reviews. Answer the following question: When comparing both, do you see any\r\n",
        "differences? Why do you think about the difference? or lack of them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9SYr0As86_b"
      },
      "source": [
        "import spacy\r\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alHj29UqFAjy"
      },
      "source": [
        "nlp.max_length = 16853000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrMWkVet887n"
      },
      "source": [
        " all_tags = []\r\n",
        "\r\n",
        "for d in positive_df['Data']:\r\n",
        "  positive_text = \" \"\r\n",
        "  positive_text = positive_text.join(d)\r\n",
        "  doc = nlp(positive_text)\r\n",
        "  for token in doc:\r\n",
        "    all_tags.append(token.pos_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPepVSeQxCIN",
        "outputId": "fa0f7e21-638e-4fe7-870c-e3b2968d5ffb"
      },
      "source": [
        "len(all_tags)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16839284"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwY65kc7xmCo"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ3WF4S5yZEk"
      },
      "source": [
        "positives = Counter(all_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjWbGOGpyrj0",
        "outputId": "5777f9bf-6ff9-44b5-ba22-3990a624ccf0"
      },
      "source": [
        "print(\"The count of nouns in the positive label : \",positives['NOUN'])\r\n",
        "print(\"The count of verbs in the positive label : \",positives['VERB'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The count of nouns in the positive label :  10099522\n",
            "The count of verbs in the positive label :  24220\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NwjYKIryspr"
      },
      "source": [
        " all_tags_neg = []\r\n",
        "\r\n",
        "for d in negative_df['Data']:\r\n",
        "  negative_text = \" \"\r\n",
        "  negative_text = negative_text.join(d)\r\n",
        "  doc = nlp(negative_text)\r\n",
        "  for token in doc:\r\n",
        "    all_tags_neg.append(token.pos_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gMX29gSJ60h",
        "outputId": "b1a2c808-5dc6-482f-abee-2974562a03e5"
      },
      "source": [
        "negatives = Counter(all_tags_neg)\r\n",
        "negatives"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'ADJ': 515,\n",
              "         'ADP': 66,\n",
              "         'ADV': 36,\n",
              "         'CCONJ': 86277,\n",
              "         'DET': 495147,\n",
              "         'INTJ': 44095,\n",
              "         'NOUN': 9716866,\n",
              "         'NUM': 40049,\n",
              "         'PART': 938,\n",
              "         'PRON': 462446,\n",
              "         'PROPN': 1822853,\n",
              "         'PUNCT': 481226,\n",
              "         'SPACE': 2873359,\n",
              "         'SYM': 65563,\n",
              "         'VERB': 22437,\n",
              "         'X': 175112})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_-6nw8nJ_JE",
        "outputId": "4d46b0f9-4a7d-4b4d-c42e-f1d93d0f18f2"
      },
      "source": [
        "print(\"The count of nouns in the positive label : \",negatives['NOUN'])\r\n",
        "print(\"The count of verbs in the positive label : \",negatives['VERB'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The count of nouns in the positive label :  9716866\n",
            "The count of verbs in the positive label :  22437\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWF5rNAONXuM"
      },
      "source": [
        "Question 8) (20 points) Using the results from the PoS process in question 7, count how many\r\n",
        "different PUNCT tags are found and their respective counts from the full dataset provided (both\r\n",
        "negative and positives together). Using regex, write a set of regular expressions that generate\r\n",
        "the same counts from the dataset without using NLTK or Spacy, just regex. Can you get the\r\n",
        "same counts? If not, why do you think this is?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gV2gwI0dVLOl",
        "outputId": "87f1413d-2a4c-4148-83b6-a4db15e997ea"
      },
      "source": [
        "punctuations_positive = positives['PUNCT']\r\n",
        "punctuations_negative = negatives['PUNCT']\r\n",
        "punctuations_total = punctuations_positive + punctuations_negative\r\n",
        "punctuations_total"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "956588"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXQnyqQ7P_r5"
      },
      "source": [
        "import re"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfLtW8yNKBY4"
      },
      "source": [
        "def count_punctuations(text): \r\n",
        "    result = re.findall(r'[!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~]', text) \r\n",
        "    return len(result)\r\n",
        "    "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b01x9JfVQe6w"
      },
      "source": [
        "st = \"\".join(map(str, data_df['Data']))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "T_PiwAh4O0bj",
        "outputId": "8ee40210-6c15-4648-8df4-4ee38760ffee"
      },
      "source": [
        "count_punctuations(st)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1327482"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8Da83Xz4Yye"
      },
      "source": [
        "For complete control over how to tokenize the text, we have regular expression.\r\n",
        "\r\n",
        "Spacy has it own set of rules to parse or tokenize the senetences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5tYn8NGlt_F"
      },
      "source": [
        "Question 5) (20 points) Build a function that takes the set of documents as input and returns a\r\n",
        "cosine similarity matrix for those documents. Feed all documents in the TRAINING folder to this\r\n",
        "matrix. Instead of printing the returned cosine similarity matrix, create a heatmap plot from the\r\n",
        "returned matrix. Make sure your plot is nicely scaled, properly labeled, and uses a nice color\r\n",
        "range to show the similarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snfN65xqtIjL"
      },
      "source": [
        "lists = []\r\n",
        "for data in data_df['Data']:\r\n",
        "  lists.append(data)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EatFMLnm7eFM"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\r\n",
        "tfidf_matrix_train = tfidf_vectorizer.fit_transform(lists)  #finds the tfidf score with normalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-R40Y9emn-a"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\r\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi49KXydsQDz"
      },
      "source": [
        "cosine_sim = cosine_similarity(tfidf_matrix_train,tfidf_matrix_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyHOP2ezyzXg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AD6Lwty7tNv"
      },
      "source": [
        "Question 6) (15 points) Write a function that takes a cosine similarity matrix as input and\r\n",
        "returns a list with the top n document paris and their similarity. Note that you should only keep\r\n",
        "the document pairs that are unique and remove the comparisons of the document to itself. Print\r\n",
        "the top 50 similar document pairs. Compare the assigned class for each document and answer:\r\n",
        "Do all similar documents belong to the same class? Why or why not?\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NpboT9z71Eo"
      },
      "source": [
        "def print_n_best(cosine_sim, n):\r\n",
        "   \r\n",
        "    sorted_sim = sorted([(i,sorted(enumerate(sim_scores), key = lambda x:x[1], reverse=True)[1]) for i, sim_scores in enumerate(cosine_sim)], key = lambda x:x[1][1], reverse = True)\r\n",
        "    sorted_sim_temp = [str(data_df['Data'][sim[0]])+\" & \"+str(data_df['data'][sim[1][0]]) for sim in sorted_sim]\r\n",
        "   \r\n",
        "    for x in sorted_sim_temp[:n]:\r\n",
        "        print(x)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-6p2J-O8F1-"
      },
      "source": [
        "print_n_best(cosine_sim,50)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}